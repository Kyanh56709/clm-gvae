{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3241f1df-8521-47f3-89aa-519e5234de5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim \n",
    "from torch_geometric.data import HeteroData, Data\n",
    "from torch_geometric.nn import (\n",
    "    GATv2Conv,        \n",
    "    Linear,            \n",
    "    LayerNorm,        \n",
    "    BatchNorm,        \n",
    "    HeteroConv        \n",
    ")\n",
    "from torch_geometric.loader import DataLoader as GraphDataLoader\n",
    "\n",
    "from torch_scatter import scatter_mean, scatter_sum, scatter_max, scatter \n",
    "# --- Scikit-learn ---\n",
    "from sklearn.model_selection import KFold, StratifiedKFold   \n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score, roc_curve\n",
    "from sklearn.preprocessing import (\n",
    "    RobustScaler,       \n",
    "    StandardScaler,     \n",
    "    OneHotEncoder      \n",
    ")\n",
    "from sklearn.metrics.pairwise import cosine_similarity \n",
    "from sklearn.neighbors import kneighbors_graph      \n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm.notebook import tqdm as tqdm_notebook \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from typing import Dict, List, Tuple, Optional, Any, Union\n",
    "\n",
    "import os\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f3f9b5-b694-4e42-82ea-7ea4718dacb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViewEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Graph VAE Encoder using GATv2Conv to map node features and graph structure\n",
    "    of a specific view to parameters of a latent Gaussian distribution (mu, logvar).\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels: int, hidden_channels: int, latent_dim: int,\n",
    "                 heads: int = 4, dropout: float = 0.5, num_gnn_layers: int = 2, edge_dim: int = -1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_channels: Dimensionality of input node features for this view.\n",
    "            hidden_channels: Dimensionality of hidden layers in the GNN.\n",
    "            latent_dim: Dimensionality of the output latent space (mu and logvar).\n",
    "            heads: Number of attention heads in GATv2Conv layers.\n",
    "            dropout: Dropout rate.\n",
    "            num_gnn_layers: Number of GATv2Conv layers (supports 1 or 2).\n",
    "            edge_dim: Dimensionality of edge features (-1 if no edge features).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        if num_gnn_layers not in [1, 2]:\n",
    "            raise ValueError(\"ViewEncoder currently supports 1 or 2 GNN layers.\")\n",
    "\n",
    "        self.num_gnn_layers = num_gnn_layers\n",
    "        self.dropout_p = dropout\n",
    "        current_dim = hidden_channels\n",
    "\n",
    "        self.conv1 = GATv2Conv(in_channels, hidden_channels, heads=heads, concat=True,\n",
    "                               dropout=dropout, edge_dim=edge_dim, add_self_loops=True)\n",
    "        self.bn1 = BatchNorm(hidden_channels * heads)\n",
    "\n",
    "        if num_gnn_layers > 1:\n",
    "            \n",
    "            self.conv2 = GATv2Conv(hidden_channels * heads, hidden_channels, heads=heads, concat=True,\n",
    "                                   dropout=dropout, edge_dim=edge_dim, add_self_loops=True)\n",
    "            self.bn2 = BatchNorm(hidden_channels * heads)\n",
    "            current_dim = hidden_channels * heads\n",
    "\n",
    "        self.fc_mu = Linear(current_dim, latent_dim)\n",
    "        self.fc_logvar = Linear(current_dim, latent_dim)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.conv1.reset_parameters()\n",
    "        self.bn1.reset_parameters()\n",
    "        if self.num_gnn_layers > 1:\n",
    "            self.conv2.reset_parameters()\n",
    "            self.bn2.reset_parameters()\n",
    "        self.fc_mu.reset_parameters()\n",
    "        self.fc_logvar.reset_parameters()\n",
    "\n",
    "    def forward(self, x: torch.Tensor, edge_index: torch.Tensor,\n",
    "                edge_attr: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Node feature matrix [num_nodes, in_channels].\n",
    "            edge_index: Graph connectivity [2, num_edges].\n",
    "            edge_attr: Edge feature matrix [num_edges, edge_dim] (optional).\n",
    "\n",
    "        Returns:\n",
    "            mu: Latent mean [num_nodes, latent_dim].\n",
    "            logvar: Latent log variance [num_nodes, latent_dim].\n",
    "        \"\"\"\n",
    "        # Layer 1\n",
    "        x = self.conv1(x, edge_index, edge_attr=edge_attr)\n",
    "        x = self.bn1(x)\n",
    "        x = F.elu(x)\n",
    "        x = F.dropout(x, p=self.dropout_p, training=self.training)\n",
    "\n",
    "        # Layer 2 (if exists)\n",
    "        if self.num_gnn_layers == 2:\n",
    "            x = self.conv2(x, edge_index, edge_attr=edge_attr)\n",
    "            x = self.bn2(x)\n",
    "            x = F.elu(x)\n",
    "            x = F.dropout(x, p=self.dropout_p, training=self.training)\n",
    "\n",
    "        # Output Projections\n",
    "        mu = self.fc_mu(x)\n",
    "        logvar = self.fc_logvar(x)\n",
    "        #logvar = torch.tanh(self.fc_logvar(x)) * 5.0\n",
    "        #logvar = F.hardtanh(self.fc_logvar(x), min_val=-6.0, max_val=2.0)\n",
    "        return mu, logvar\n",
    "\n",
    "# --- 2. Structure Decoder (Adjacency Reconstruction) ---\n",
    "class StructureDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Decodes latent embeddings to reconstruct graph adjacency matrix logits\n",
    "    using inner product.\n",
    "    \"\"\"\n",
    "    def __init__(self, activation: str = 'none'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            activation: Output activation ('sigmoid' or 'none'). 'none' is suitable\n",
    "                        for BCEWithLogitsLoss.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        if activation not in ['sigmoid', 'none']:\n",
    "             raise ValueError(\"Activation must be 'sigmoid' or 'none'\")\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            z: Latent node embeddings [num_nodes, latent_dim].\n",
    "\n",
    "        Returns:\n",
    "            adj_rec_logits or adj_rec_probs: Reconstructed adjacency [num_nodes, num_nodes].\n",
    "        \"\"\"\n",
    "        adj_rec_logits = torch.matmul(z, z.t())\n",
    "        if self.activation == 'sigmoid':\n",
    "            return torch.sigmoid(adj_rec_logits)\n",
    "        return adj_rec_logits\n",
    "\n",
    "# --- 3. Attribute Decoder (Feature Reconstruction) ---\n",
    "class AttributeDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Decodes latent embeddings back to the original node feature space using an MLP.\n",
    "    Applies Tanh activation to the output to control scale and prevent explosion.\n",
    "    \"\"\"\n",
    "    def __init__(self, latent_dim: int, original_feature_dim: int, hidden_decoder_dim: Optional[int] = None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            latent_dim: Dimensionality of the latent embeddings.\n",
    "            original_feature_dim: Dimensionality of the original node features to reconstruct.\n",
    "            hidden_decoder_dim: Dimensionality of the hidden layer in the MLP decoder.\n",
    "                                Defaults to latent_dim if None.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        if hidden_decoder_dim is None:\n",
    "            hidden_decoder_dim = latent_dim\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            Linear(latent_dim, hidden_decoder_dim),\n",
    "            nn.ReLU(),\n",
    "            #nn.ELU(),\n",
    "            Linear(hidden_decoder_dim, original_feature_dim),\n",
    "        )\n",
    "        self.norm_layer = LayerNorm(original_feature_dim)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "         for layer in self.mlp:\n",
    "             if hasattr(layer, 'reset_parameters'):\n",
    "                 layer.reset_parameters()\n",
    "\n",
    "    def forward(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        x_hat = self.mlp(z)\n",
    "        return self.norm_layer(x_hat)\n",
    "\n",
    "# --- 4. Attention Fusion Layer ---\n",
    "class AttentionFusionLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Fuses embeddings from multiple views using an attention mechanism.\n",
    "    Takes view embeddings stacked along a dimension.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim: int, num_views: int, hidden_dim_attention: int, dropout: float = 0.3):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embed_dim: Dimensionality of the input embeddings from each view.\n",
    "            num_views: Number of views being fused.\n",
    "            hidden_dim_attention: Hidden dimension for the attention MLP.\n",
    "            dropout: Dropout rate applied to the fused embedding.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_views = num_views\n",
    "\n",
    "        self.attention_mlp = nn.Sequential(\n",
    "            # Input is concatenation of all view embeddings\n",
    "            Linear(embed_dim * num_views, hidden_dim_attention),\n",
    "            nn.Tanh(),\n",
    "            Linear(hidden_dim_attention, num_views) \n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.final_norm = LayerNorm(embed_dim) \n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "         for layer in self.attention_mlp:\n",
    "             if hasattr(layer, 'reset_parameters'):\n",
    "                 layer.reset_parameters()\n",
    "         self.final_norm.reset_parameters()\n",
    "\n",
    "    def forward(self, view_embeddings_stacked: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            view_embeddings_stacked: Tensor of view embeddings, shape [batch_size, num_views, embed_dim].\n",
    "\n",
    "        Returns:\n",
    "            fused_embedding: Fused representation [batch_size, embed_dim].\n",
    "            attention_weights: Attention weights applied [batch_size, num_views].\n",
    "        \"\"\"\n",
    "        batch_size = view_embeddings_stacked.shape[0]\n",
    "\n",
    "        # Reshape for attention MLP: [batch_size, num_views * embed_dim]\n",
    "        concatenated_embeddings = view_embeddings_stacked.view(batch_size, -1)\n",
    "\n",
    "        # Get attention scores\n",
    "        attn_scores = self.attention_mlp(concatenated_embeddings) # [batch_size, num_views]\n",
    "\n",
    "        # Apply softmax to get attention weights\n",
    "        attention_weights = F.softmax(attn_scores, dim=1) # [batch_size, num_views]\n",
    "\n",
    "        # Calculate weighted sum using original embeddings\n",
    "        # Reshape weights to allow broadcasting: [batch_size, num_views, 1]\n",
    "        attention_weights_expanded = attention_weights.unsqueeze(-1)\n",
    "\n",
    "        # Element-wise multiplication and sum over the num_views dimension\n",
    "        fused_embedding = (view_embeddings_stacked * attention_weights_expanded).sum(dim=1) \n",
    "\n",
    "        fused_embedding = self.dropout(fused_embedding)\n",
    "        fused_embedding = self.final_norm(fused_embedding)\n",
    "\n",
    "        return fused_embedding, attention_weights\n",
    "\n",
    "\n",
    "# --- 5. Classifier MLP ---\n",
    "class ClassifierMLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple MLP for binary classification based on the fused embedding.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, output_dim: int = 1, dropout: float = 0.5):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_dim: Dimensionality of the fused input embedding.\n",
    "            hidden_dim: Dimensionality of the hidden layer.\n",
    "            output_dim: Dimensionality of the output (1 for binary classification logits).\n",
    "            dropout: Dropout rate.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            Linear(hidden_dim, output_dim) \n",
    "        )\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "         for layer in self.mlp:\n",
    "             if hasattr(layer, 'reset_parameters'):\n",
    "                 layer.reset_parameters()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Fused input embedding [batch_size, input_dim].\n",
    "\n",
    "        Returns:\n",
    "            logits: Classification logits [batch_size, output_dim].\n",
    "        \"\"\"\n",
    "        return self.mlp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2659b07-1e63-4c6f-ad32-82427a107e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MHA_CLSToken_FusionLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Fuses view embeddings using a learnable [CLS] token and Multi-Head Self-Attention.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim: int, num_heads: int, \n",
    "                 ffn_dim_multiplier: int = 2, dropout: float = 0.1, \n",
    "                 output_dim: Optional[int] = None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embed_dim: Dimensionality of the input view embeddings.\n",
    "            num_heads: Number of attention heads.\n",
    "            ffn_dim_multiplier: Multiplier for the feed-forward layer's hidden dim.\n",
    "            dropout: Dropout rate.\n",
    "            output_dim: Final dimension of the fused embedding. Defaults to embed_dim.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim if output_dim is not None else embed_dim\n",
    "\n",
    "        # 1. The learnable [CLS] token\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
    "\n",
    "        # 2. The Multi-Head Attention layer\n",
    "        self.mha = nn.MultiheadAttention(\n",
    "            embed_dim=embed_dim, \n",
    "            num_heads=num_heads, \n",
    "            dropout=dropout, \n",
    "            batch_first=True \n",
    "        )\n",
    "        \n",
    "        # 3. A standard Feed-Forward Network (part of a Transformer block)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim * ffn_dim_multiplier),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(embed_dim * ffn_dim_multiplier, embed_dim)\n",
    "        )\n",
    "\n",
    "        # 4. Layer Normalization\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        # 5. Optional final projection layer\n",
    "        self.final_projection = nn.Linear(embed_dim, self.output_dim) if embed_dim != self.output_dim else nn.Identity()\n",
    "        \n",
    "    def forward(self, view_embeddings_stacked: torch.Tensor) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            view_embeddings_stacked: Tensor of view embeddings, shape [batch_size, num_views, embed_dim].\n",
    "        \n",
    "        Returns:\n",
    "            fused_embedding: A single fused vector per patient, shape [batch_size, output_dim].\n",
    "            attention_weights: None, as extracting them is complex and not the primary goal.\n",
    "        \"\"\"\n",
    "        batch_size = view_embeddings_stacked.shape[0]\n",
    "\n",
    "        # Prepend the CLS token to the sequence of view embeddings\n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
    "        x = torch.cat((cls_tokens, view_embeddings_stacked), dim=1) # Shape: [batch_size, num_views + 1, embed_dim]\n",
    "\n",
    "        # --- First part of Transformer Block: MHA + Residual + Norm ---\n",
    "        # Self-attention: query, key, and value are all the same\n",
    "        attn_output, _ = self.mha(x, x, x)\n",
    "        # Residual connection\n",
    "        x = x + attn_output\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        # --- Second part of Transformer Block: FFN + Residual + Norm ---\n",
    "        ffn_output = self.ffn(x)\n",
    "        # Residual connection\n",
    "        x = x + ffn_output\n",
    "        x = self.norm2(x)\n",
    "        \n",
    "        # The final fused representation is the output of the CLS token (at position 0)\n",
    "        cls_output = x[:, 0, :] # Shape: [batch_size, embed_dim]\n",
    "\n",
    "        # Apply final projection\n",
    "        fused_embedding = self.final_projection(cls_output)\n",
    "\n",
    "        return fused_embedding, None # Return None for attention weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec2901fe-ca99-4b32-b496-72c34f3236fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear, LayerNorm # Ensure Linear is imported if not already\n",
    "\n",
    "class ProjectionHead(nn.Module):\n",
    "    \"\"\"\n",
    "    Projects embeddings (typically mu from VAE) to a new space for contrastive learning.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, output_dim: int, dropout: float = 0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_dim: Dimensionality of the input embeddings (e.g., d_embed).\n",
    "            hidden_dim: Dimensionality of the hidden layer.\n",
    "            output_dim: Dimensionality of the projected embeddings for CL.\n",
    "            dropout: Dropout rate.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            Linear(input_dim, hidden_dim),\n",
    "            LayerNorm(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for layer in self.net:\n",
    "            if hasattr(layer, 'reset_parameters'):\n",
    "                layer.reset_parameters()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input embeddings [batch_size, input_dim] or [num_nodes, input_dim].\n",
    "\n",
    "        Returns:\n",
    "            Projected and L2-normalized embeddings [batch_size, output_dim].\n",
    "        \"\"\"\n",
    "        projected_x = self.net(x)\n",
    "        return F.normalize(projected_x, p=2, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e92770-07a8-4a2b-8b50-4aa665850b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_contrastive_loss(\n",
    "    sampled_zs_per_view_batch: Dict[str, Tuple[torch.Tensor, torch.Tensor]],\n",
    "    temperature: float\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Calculates cross-view contrastive loss for patients present in multiple views within the batch.\n",
    "    Args:\n",
    "        sampled_zs_per_view_batch: Dict where key is view_name, value is a tuple:\n",
    "                                   (Tensor of z_embeddings for patients in batch having this view,\n",
    "                                    Tensor of global_indices for these patients).\n",
    "        temperature: Temperature for InfoNCE.\n",
    "    Returns:\n",
    "        Contrastive loss scalar.\n",
    "    \"\"\"\n",
    "    if sampled_zs_per_view_batch:\n",
    "        first_emb = list(sampled_zs_per_view_batch.values())[0][0]\n",
    "        if first_emb is not None:\n",
    "            device = first_emb.device\n",
    "\n",
    "    total_contrastive_loss = torch.tensor(0.0, device=device)\n",
    "    num_contrastive_pairs_total = 0\n",
    "\n",
    "    # Create a list of (global_patient_idx, view_name, embedding_tensor)\n",
    "    all_embeddings_flat = []\n",
    "    for view_name, (embeddings, global_indices) in sampled_zs_per_view_batch.items():\n",
    "        if embeddings is not None and global_indices is not None and embeddings.numel() > 0: # Check if embeddings exist\n",
    "            for i in range(embeddings.shape[0]):\n",
    "                all_embeddings_flat.append((global_indices[i].item(), view_name, embeddings[i]))\n",
    "\n",
    "    if not all_embeddings_flat:\n",
    "        return total_contrastive_loss\n",
    "\n",
    "    # Group embeddings by global_patient_idx\n",
    "    patient_to_view_embeddings = {}\n",
    "    for global_idx, view_name, emb in all_embeddings_flat:\n",
    "        if global_idx not in patient_to_view_embeddings:\n",
    "            patient_to_view_embeddings[global_idx] = []\n",
    "        patient_to_view_embeddings[global_idx].append(emb)\n",
    "\n",
    "    # For each patient with embeddings from multiple views\n",
    "    for global_idx, view_embs_list in patient_to_view_embeddings.items():\n",
    "        if len(view_embs_list) < 2: # Need at least two views for this patient\n",
    "            continue\n",
    "\n",
    "        # Form positive pairs for this patient\n",
    "        for i in range(len(view_embs_list)):\n",
    "            for j in range(i + 1, len(view_embs_list)):\n",
    "                z_i = view_embs_list[i].unsqueeze(0) # Anchor [1, d_embed]\n",
    "                z_j = view_embs_list[j].unsqueeze(0) # Positive [1, d_embed]\n",
    "\n",
    "                # Negative samples: all other embeddings in all_embeddings_flat NOT from this patient\n",
    "                negatives = torch.stack([\n",
    "                    other_emb for other_global_idx, _, other_emb in all_embeddings_flat\n",
    "                    if other_global_idx != global_idx\n",
    "                ])\n",
    "\n",
    "                if negatives.numel() == 0: # Only one patient in batch, no negatives\n",
    "                    continue\n",
    "\n",
    "                # Cosine similarity\n",
    "                sim_positive = F.cosine_similarity(z_i, z_j, dim=1) / temperature # Shape [1]\n",
    "                sim_negatives = F.cosine_similarity(z_i.expand(negatives.shape[0], -1), negatives, dim=1) / temperature # Shape [num_negatives]\n",
    "\n",
    "                # Concatenate positive score with negative scores for logits\n",
    "                logits = torch.cat([sim_positive, sim_negatives]) # Shape [1 + num_negatives]\n",
    "                target_index = torch.tensor([0], device=device, dtype=torch.long) \n",
    "\n",
    "\n",
    "                total_contrastive_loss += F.cross_entropy(logits.unsqueeze(0), target_index)\n",
    "                num_contrastive_pairs_total += 1\n",
    "\n",
    "    return total_contrastive_loss / num_contrastive_pairs_total if num_contrastive_pairs_total > 0 else torch.tensor(0.0, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a291edcb-6670-41db-8171-6b8b7c84a32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_view_subgraph_and_features(\n",
    "    full_data: HeteroData,\n",
    "    view_name: str,\n",
    "    batch_patient_global_indices: torch.Tensor\n",
    ") -> Tuple[Optional[torch.Tensor], Optional[torch.Tensor], Optional[torch.Tensor], Optional[torch.Tensor]]:\n",
    "    \"\"\"\n",
    "    Extracts features and a local subgraph for a specific view.\n",
    "    For 'radiology', x_view_subset_batch will be None as features are handled by lesion aggregator.\n",
    "    It still extracts the patient-patient similarity graph for radiology if it exists.\n",
    "    \"\"\"\n",
    "    device = batch_patient_global_indices.device\n",
    "    x_view_feature_key = f'x_{view_name}'\n",
    "    edge_type_sim = ('patient', f'similar_to_{view_name}', 'patient')\n",
    "\n",
    "    x_view_subset_batch: Optional[torch.Tensor] = None # Initialize\n",
    "    global_indices_of_subset_in_batch: torch.Tensor = batch_patient_global_indices \n",
    "\n",
    "    if view_name == 'radiology':\n",
    "        mask_feature_key = f'{view_name}_mask'\n",
    "        if mask_feature_key not in full_data['patient']:\n",
    "            print(f\"Warning: Mask key '{mask_feature_key}' not found for view '{view_name}'. Assuming no patients have this view in batch.\")\n",
    "            return None, torch.empty((2,0), dtype=torch.long, device=device), None, torch.empty(0, dtype=torch.long, device=device)\n",
    "\n",
    "        view_presence_mask_all_patients = full_data['patient'][mask_feature_key]\n",
    "        view_presence_in_batch = view_presence_mask_all_patients[batch_patient_global_indices]\n",
    "        global_indices_of_subset_in_batch = batch_patient_global_indices[view_presence_in_batch]\n",
    "\n",
    "        if global_indices_of_subset_in_batch.numel() == 0:\n",
    "            return None, torch.empty((2,0), dtype=torch.long, device=device), None, global_indices_of_subset_in_batch\n",
    "\n",
    "    elif view_name == 'clinical':\n",
    "        if x_view_feature_key not in full_data['patient']:\n",
    "             print(f\"Warning: Clinical features '{x_view_feature_key}' not found.\")\n",
    "             return None, torch.empty((2,0), dtype=torch.long, device=device), None, torch.empty(0, dtype=torch.long, device=device)\n",
    "        global_indices_of_subset_in_batch = batch_patient_global_indices\n",
    "        x_view_subset_batch = full_data['patient'][x_view_feature_key][global_indices_of_subset_in_batch]\n",
    "\n",
    "    else:\n",
    "        mask_feature_key = f'{view_name}_mask'\n",
    "        if mask_feature_key not in full_data['patient']:\n",
    "            print(f\"Warning: Mask key '{mask_feature_key}' not found for view '{view_name}'.\")\n",
    "            return None, torch.empty((2,0), dtype=torch.long, device=device), None, torch.empty(0, dtype=torch.long, device=device)\n",
    "        if x_view_feature_key not in full_data['patient']:\n",
    "             print(f\"Warning: Features '{x_view_feature_key}' not found for view '{view_name}'.\")\n",
    "             return None, torch.empty((2,0), dtype=torch.long, device=device), None, torch.empty(0, dtype=torch.long, device=device)\n",
    "\n",
    "        view_presence_mask_all_patients = full_data['patient'][mask_feature_key]\n",
    "        view_presence_in_batch = view_presence_mask_all_patients[batch_patient_global_indices]\n",
    "        global_indices_of_subset_in_batch = batch_patient_global_indices[view_presence_in_batch]\n",
    "\n",
    "        if global_indices_of_subset_in_batch.numel() == 0:\n",
    "            return None, torch.empty((2,0), dtype=torch.long, device=device), None, global_indices_of_subset_in_batch\n",
    "        x_view_subset_batch = full_data['patient'][x_view_feature_key][global_indices_of_subset_in_batch]\n",
    "\n",
    "\n",
    "    if edge_type_sim not in full_data.edge_types:\n",
    "        # print(f\"Warning: Similarity edge type '{edge_type_sim}' not found for view '{view_name}\")\n",
    "        return x_view_subset_batch, torch.empty((2,0), dtype=torch.long, device=device), None, global_indices_of_subset_in_batch\n",
    "\n",
    "    view_full_edge_index = full_data[edge_type_sim].edge_index\n",
    "    view_full_edge_attr = getattr(full_data[edge_type_sim], 'edge_attr', None)\n",
    "\n",
    "    num_batch_patients_with_view_for_sim_graph = global_indices_of_subset_in_batch.numel()\n",
    "    if num_batch_patients_with_view_for_sim_graph == 0: #\n",
    "        return x_view_subset_batch, torch.empty((2,0), dtype=torch.long, device=device), None, global_indices_of_subset_in_batch\n",
    "\n",
    "    global_to_local_idx_map_sim_graph = {global_idx.item(): local_idx for local_idx, global_idx in enumerate(global_indices_of_subset_in_batch)}\n",
    "\n",
    "    src_nodes_global = view_full_edge_index[0]\n",
    "    dst_nodes_global = view_full_edge_index[1]\n",
    "    mask_src_in_subset = torch.isin(src_nodes_global, global_indices_of_subset_in_batch)\n",
    "    mask_dst_in_subset = torch.isin(dst_nodes_global, global_indices_of_subset_in_batch)\n",
    "    edge_selection_mask = mask_src_in_subset & mask_dst_in_subset\n",
    "\n",
    "    if not edge_selection_mask.any():\n",
    "        empty_edge_index = torch.empty((2,0), dtype=torch.long, device=device)\n",
    "        empty_edge_attr = None\n",
    "        if view_full_edge_attr is not None:\n",
    "             empty_edge_attr = torch.empty((0, view_full_edge_attr.shape[1]), dtype=view_full_edge_attr.dtype, device=device)\n",
    "        return x_view_subset_batch, empty_edge_index, empty_edge_attr, global_indices_of_subset_in_batch\n",
    "\n",
    "    selected_edges_global_src = src_nodes_global[edge_selection_mask]\n",
    "    selected_edges_global_dst = dst_nodes_global[edge_selection_mask]\n",
    "\n",
    "    try:\n",
    "        local_edge_src = torch.tensor([global_to_local_idx_map_sim_graph[idx.item()] for idx in selected_edges_global_src], dtype=torch.long, device=device)\n",
    "        local_edge_dst = torch.tensor([global_to_local_idx_map_sim_graph[idx.item()] for idx in selected_edges_global_dst], dtype=torch.long, device=device)\n",
    "        local_edge_index_batch_sim_graph = torch.stack([local_edge_src, local_edge_dst], dim=0)\n",
    "    except KeyError as e:\n",
    "         print(f\"Error remapping indices for view {view_name} similarity graph. Missing global index in map: {e}. Returning no edges.\")\n",
    "         empty_edge_index = torch.empty((2,0), dtype=torch.long, device=device)\n",
    "         empty_edge_attr = None\n",
    "         if view_full_edge_attr is not None:\n",
    "             empty_edge_attr = torch.empty((0, view_full_edge_attr.shape[1]), dtype=view_full_edge_attr.dtype, device=device)\n",
    "         return x_view_subset_batch, empty_edge_index, empty_edge_attr, global_indices_of_subset_in_batch\n",
    "    \n",
    "    local_edge_attr_batch_sim_graph = None\n",
    "    if view_full_edge_attr is not None:\n",
    "        local_edge_attr_batch_sim_graph = view_full_edge_attr[edge_selection_mask]\n",
    "\n",
    "    return x_view_subset_batch, local_edge_index_batch_sim_graph, local_edge_attr_batch_sim_graph, global_indices_of_subset_in_batch\n",
    "\n",
    "def get_dense_adj_for_reconstruction(local_edge_index: Optional[torch.Tensor], num_nodes_in_subset: int, device: torch.device) -> torch.Tensor:\n",
    "    \"\"\"Creates a dense adjacency matrix from local_edge_index for reconstruction loss.\"\"\"\n",
    "    adj = torch.zeros((num_nodes_in_subset, num_nodes_in_subset), device=device)\n",
    "    if local_edge_index is not None and local_edge_index.numel() > 0:\n",
    "        adj[local_edge_index[0], local_edge_index[1]] = 1\n",
    "        #make symmetric\n",
    "        adj = torch.max(adj, adj.t()) \n",
    "    return adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fd287d-f0a0-43d9-927f-0bf3217304c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_scatter import scatter_mean, scatter_add, scatter_max \n",
    "\n",
    "class RadiologyLesionAttentionAggregator(nn.Module):\n",
    "    def __init__(self, lesion_feature_dim: int, patient_embed_dim: int,\n",
    "                 attention_hidden_dim: Optional[int] = None, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.lesion_feature_dim = lesion_feature_dim\n",
    "        self.patient_embed_dim = patient_embed_dim\n",
    "        \n",
    "        if attention_hidden_dim is None:\n",
    "            attention_hidden_dim = lesion_feature_dim\n",
    "\n",
    "        # Attention mechanism: learns to score lesions\n",
    "        # Takes individual lesion features\n",
    "        self.attention_mlp = nn.Sequential(\n",
    "            nn.Linear(lesion_feature_dim, attention_hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(attention_hidden_dim, 1) \n",
    "        )\n",
    "\n",
    "        if lesion_feature_dim != patient_embed_dim:\n",
    "            self.output_projection = nn.Linear(lesion_feature_dim, patient_embed_dim)\n",
    "        else:\n",
    "            self.output_projection = nn.Identity()\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.norm_layer = nn.LayerNorm(patient_embed_dim) # Normalize the final patient embedding\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for layer in self.attention_mlp:\n",
    "            if hasattr(layer, 'reset_parameters'):\n",
    "                layer.reset_parameters()\n",
    "        if isinstance(self.output_projection, nn.Linear):\n",
    "            self.output_projection.reset_parameters()\n",
    "        self.norm_layer.reset_parameters()\n",
    "\n",
    "    def forward(self, lesion_x: torch.Tensor, patient_to_lesion_edge_index: torch.Tensor,\n",
    "                num_patients_in_batch: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Aggregates lesion features for patients using attention.\n",
    "\n",
    "        Args:\n",
    "            lesion_x: Tensor of lesion features [total_lesions_in_batch, lesion_feature_dim].\n",
    "            patient_to_lesion_edge_index: Edge index [2, num_edges] connecting\n",
    "                                           batch-local patient indices to batch-local lesion indices.\n",
    "                                           edge_index[0] = batch_local_patient_idx\n",
    "                                           edge_index[1] = batch_local_lesion_idx\n",
    "            num_patients_in_batch: The number of unique patients in this batch for whom\n",
    "                                   we need to produce aggregated embeddings.\n",
    "\n",
    "        Returns:\n",
    "            patient_radiology_embeddings: Tensor [num_patients_in_batch, patient_embed_dim].\n",
    "                                          Contains aggregated features for patients who have lesions.\n",
    "                                          For patients with no lesions, their rows will be zeros.\n",
    "        \"\"\"\n",
    "        if lesion_x.numel() == 0 or patient_to_lesion_edge_index.numel() == 0:\n",
    "            # No lesions in this batch, return zeros for all patients\n",
    "            return torch.zeros((num_patients_in_batch, self.patient_embed_dim),\n",
    "                               device=lesion_x.device, dtype=lesion_x.dtype)\n",
    "\n",
    "        batch_local_patient_indices = patient_to_lesion_edge_index[0]\n",
    "        batch_local_lesion_indices = patient_to_lesion_edge_index[1] \n",
    "\n",
    "        relevant_lesion_features = lesion_x[batch_local_lesion_indices]\n",
    "\n",
    "        # 1. Calculate attention scores for each lesion\n",
    "        attn_scores = self.attention_mlp(relevant_lesion_features)  # [num_batch_edges, 1]\n",
    "\n",
    "        # 2. Apply softmax grouped by patient to get attention weights\n",
    "\n",
    "        attn_scores_max_per_patient = scatter_max(attn_scores.squeeze(-1), batch_local_patient_indices, dim=0, dim_size=num_patients_in_batch)[0]\n",
    "        attn_scores_stabilized = attn_scores.squeeze(-1) - attn_scores_max_per_patient[batch_local_patient_indices]\n",
    "        \n",
    "        attn_exp = torch.exp(attn_scores_stabilized)\n",
    "        attn_exp_sum_per_patient = scatter_add(attn_exp, batch_local_patient_indices, dim=0, dim_size=num_patients_in_batch)\n",
    "        \n",
    "        attn_exp_sum_per_patient = attn_exp_sum_per_patient.clamp(min=1e-12) \n",
    "        \n",
    "        alpha = attn_exp / attn_exp_sum_per_patient[batch_local_patient_indices] # [num_batch_edges]\n",
    "        alpha = alpha.unsqueeze(-1) # [num_batch_edges, 1]\n",
    "\n",
    "        # 3. Calculate weighted sum of lesion features for each patient\n",
    "        weighted_lesion_features = relevant_lesion_features * alpha # [num_batch_edges, lesion_feature_dim]\n",
    "        \n",
    "        # Aggregate weighted features per patient\n",
    "        aggregated_patient_features = scatter_add(\n",
    "            weighted_lesion_features, batch_local_patient_indices, dim=0, dim_size=num_patients_in_batch\n",
    "        ) # [num_patients_in_batch, lesion_feature_dim]\n",
    "\n",
    "        # 4. Optional output projection and normalization\n",
    "        projected_features = self.output_projection(aggregated_patient_features)\n",
    "        projected_features = self.dropout(projected_features)\n",
    "        normalized_features = self.norm_layer(projected_features)\n",
    "        \n",
    "        return normalized_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc518b1-7557-4a06-a21f-1a515041ec56",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EndToEndMultiViewVAE_CL_AttentionRadiology(nn.Module):\n",
    "    def __init__(self, \n",
    "                 view_configs: Dict[str, Any], \n",
    "                 radiology_aggregator_config: Dict[str, Any],\n",
    "                 projection_head_config: Dict[str, Any],\n",
    "                 fusion_config: Dict[str, Any], \n",
    "                 classifier_config: Dict[str, Any], \n",
    "                 d_embed: int,\n",
    "                 missing_strategy: str ='zero'):\n",
    "        super().__init__()\n",
    "        self.views = list(view_configs.keys())\n",
    "        self.d_embed = d_embed\n",
    "        self.missing_strategy = missing_strategy\n",
    "\n",
    "        self.radiology_lesion_aggregator = None\n",
    "        if 'radiology' in self.views and radiology_aggregator_config:\n",
    "            self.radiology_lesion_aggregator = RadiologyLesionAttentionAggregator(\n",
    "                lesion_feature_dim=radiology_aggregator_config['lesion_feature_dim'],\n",
    "                patient_embed_dim=radiology_aggregator_config['aggregated_output_dim'],\n",
    "                attention_hidden_dim=radiology_aggregator_config.get('attention_hidden_dim'),\n",
    "                dropout=radiology_aggregator_config.get('dropout', 0.1)\n",
    "            )\n",
    "            if view_configs['radiology']['in_channels'] != radiology_aggregator_config['aggregated_output_dim']:\n",
    "                raise ValueError(\"Radiology VAE in_channels must match aggregator output_dim\")\n",
    "        \n",
    "        # --- VAE Components ---\n",
    "        self.vae_encoders = nn.ModuleDict()\n",
    "        self.structure_decoders = nn.ModuleDict()\n",
    "        self.attribute_decoders = nn.ModuleDict()\n",
    "        if self.missing_strategy == 'learnable':\n",
    "             self.missing_embeddings_params = nn.ParameterDict()\n",
    "\n",
    "        self.projection_heads = nn.ModuleDict()\n",
    "\n",
    "        for view, config in view_configs.items():\n",
    "            self.vae_encoders[view] = ViewEncoder(\n",
    "                config['in_channels'], config['hidden_channels_vae'], d_embed,\n",
    "                config.get('heads', 4), config.get('dropout', 0.3),\n",
    "                config.get('num_gnn_layers_vae', 2), config.get('edge_dim', -1)\n",
    "            )\n",
    "            self.structure_decoders[view] = StructureDecoder()\n",
    "            self.attribute_decoders[view] = AttributeDecoder(d_embed, config['in_channels'])\n",
    "\n",
    "            self.projection_heads[view] = ProjectionHead(\n",
    "                input_dim=d_embed,\n",
    "                hidden_dim=projection_head_config.get('hidden_dim', d_embed),\n",
    "                output_dim=projection_head_config.get('output_dim', d_embed),\n",
    "                dropout=projection_head_config.get('dropout', 0.1)\n",
    "            )\n",
    "            if self.missing_strategy == 'learnable':\n",
    "                self.missing_embeddings_params[view] = nn.Parameter(torch.randn(1, d_embed))\n",
    "\n",
    "        self.fusion_layer = MHA_CLSToken_FusionLayer(\n",
    "                embed_dim=d_embed, # The dimension of your view embeddings\n",
    "                num_heads=fusion_config.get('num_fusion_heads', 4),\n",
    "                ffn_dim_multiplier=fusion_config.get('fusion_ffn_multiplier', 2),\n",
    "                dropout=fusion_config.get('dropout_fusion', 0.1),\n",
    "                output_dim=fusion_config.get('fused_dim', d_embed)\n",
    "        )\n",
    "        \n",
    "        self.classifier = ClassifierMLP(\n",
    "                input_dim=fusion_config.get('fused_dim', d_embed),\n",
    "                hidden_dim=classifier_config['hidden_dim_classifier'],\n",
    "                output_dim=1, dropout=classifier_config.get('dropout_class', 0.5)\n",
    "        )\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        if self.training:\n",
    "            std = torch.exp(0.5 * logvar)\n",
    "            eps = torch.randn_like(std)\n",
    "            return mu + eps * std\n",
    "        else:\n",
    "            return mu\n",
    "\n",
    "    def forward(self, full_data: HeteroData, batch_patient_global_indices: torch.Tensor):\n",
    "        device = batch_patient_global_indices.device\n",
    "\n",
    "        vae_outputs_for_loss = {view: {} for view in self.views}\n",
    "        # all_patient_view_zs_for_fusion = {\n",
    "        #     idx.item(): {view: torch.zeros(self.d_embed, device=device) for view in self.views}\n",
    "        #     for idx in batch_patient_global_indices\n",
    "        # }\n",
    "        all_patient_zs_for_fusion = {}\n",
    "        \n",
    "        #mu_projected' embeddings for the contrastive loss\n",
    "        all_patient_mus_projected_for_cl = {}\n",
    "\n",
    "        for view in self.views:\n",
    "            x_patient_level_subset, local_patient_sim_edge_idx, local_patient_sim_edge_attr, global_indices_subset_patients = \\\n",
    "                get_view_subgraph_and_features(full_data, view, batch_patient_global_indices)\n",
    "            \n",
    "            if global_indices_subset_patients.numel() == 0:\n",
    "                vae_outputs_for_loss[view] = { # Ensure structure exists for loss calculation\n",
    "                    'mu': None, 'logvar': None, 'z_sampled_for_dec': None,\n",
    "                    'rec_adj_logits': None, 'rec_x': None,\n",
    "                    'original_x_subset': None, 'original_adj_subset': None\n",
    "                }\n",
    "                continue\n",
    "\n",
    "            num_active_patients_for_view = global_indices_subset_patients.shape[0]\n",
    "            x_for_vae_encoder, original_x_for_vae_reconstruction = None, None\n",
    "            if view == 'radiology' and self.radiology_lesion_aggregator:\n",
    "                                \n",
    "                patient_lesion_edges_all = full_data['patient', 'has_lesion', 'lesion'].edge_index\n",
    "                all_lesion_features_all = full_data['lesion'].x\n",
    "\n",
    "                # Map active global patient indices to their 0-based local indices within this subset\n",
    "                active_patient_global_to_local_map = {glob_idx.item(): i for i, glob_idx in enumerate(global_indices_subset_patients)}\n",
    "                \n",
    "                batch_lesion_src_patient_local_idx_list = []\n",
    "                batch_lesion_node_global_idx_list = []\n",
    "\n",
    "                for i_edge in range(patient_lesion_edges_all.shape[1]):\n",
    "                    src_patient_global = patient_lesion_edges_all[0, i_edge].item()\n",
    "                    dst_lesion_global = patient_lesion_edges_all[1, i_edge].item()\n",
    "                    if src_patient_global in active_patient_global_to_local_map:\n",
    "                        batch_lesion_src_patient_local_idx_list.append(active_patient_global_to_local_map[src_patient_global])\n",
    "                        batch_lesion_node_global_idx_list.append(dst_lesion_global)\n",
    "                \n",
    "                if batch_lesion_node_global_idx_list:\n",
    "                    batch_lesion_src_patient_local_idx_t = torch.tensor(batch_lesion_src_patient_local_idx_list, dtype=torch.long, device=device)\n",
    "                    batch_lesion_node_global_idx_t = torch.tensor(batch_lesion_node_global_idx_list, dtype=torch.long, device=device)\n",
    "                    \n",
    "                    lesion_features_for_batch_agg = all_lesion_features_all[batch_lesion_node_global_idx_t]\n",
    "                    unique_lesions_in_batch, inverse_indices = torch.unique(batch_lesion_node_global_idx_t, return_inverse=True)\n",
    "                    batch_local_lesion_indices_for_agg = torch.arange(lesion_features_for_batch_agg.shape[0], device=device)\n",
    "\n",
    "                    patient_to_batch_lesion_edge_index = torch.stack([\n",
    "                        batch_lesion_src_patient_local_idx_t, # patient indices (local to current subset)\n",
    "                        batch_local_lesion_indices_for_agg    # lesion indices (local to lesions_for_batch_agg)\n",
    "                    ], dim=0)\n",
    "\n",
    "                    x_for_vae_encoder = self.radiology_lesion_aggregator(\n",
    "                        lesion_features_for_batch_agg,\n",
    "                        patient_to_batch_lesion_edge_index,\n",
    "                        num_active_patients_for_view \n",
    "                    )\n",
    "                    original_x_for_vae_reconstruction = x_for_vae_encoder\n",
    "                \n",
    "                pass # Assume this part is correctly implemented\n",
    "            elif x_patient_level_subset is not None and x_patient_level_subset.numel() > 0:\n",
    "                x_for_vae_encoder = x_patient_level_subset\n",
    "                original_x_for_vae_reconstruction = x_for_vae_encoder\n",
    "\n",
    "            if x_for_vae_encoder is not None and x_for_vae_encoder.numel() > 0:\n",
    "                num_nodes_for_vae = x_for_vae_encoder.shape[0]\n",
    "                # 1. Get mu and logvar from the VAE encoder\n",
    "                mu, logvar = self.vae_encoders[view](x_for_vae_encoder, local_patient_sim_edge_idx, local_patient_sim_edge_attr)\n",
    "                # 2. Get z_sampled (noisy version) for reconstruction and fusion\n",
    "                z_sampled = self.reparameterize(mu, logvar)\n",
    "\n",
    "                mu_projected = self.projection_heads[view](mu)\n",
    "\n",
    "                # Store for VAE loss\n",
    "                vae_outputs_for_loss[view] = {\n",
    "                    'mu': mu, 'logvar': logvar, 'z_sampled_for_dec': z_sampled,\n",
    "                    'original_x_subset': original_x_for_vae_reconstruction,\n",
    "                    'original_adj_subset': get_dense_adj_for_reconstruction(local_patient_sim_edge_idx, num_nodes_for_vae, device),\n",
    "                    'rec_adj_logits': self.structure_decoders[view](z_sampled),\n",
    "                    'rec_x': self.attribute_decoders[view](z_sampled)\n",
    "                }\n",
    "                \n",
    "                # 5. Gather embeddings for downstream tasks\n",
    "                for i, global_idx_tensor in enumerate(global_indices_subset_patients):\n",
    "                    global_idx_item = global_idx_tensor.item()\n",
    "                    \n",
    "                    # Store z_sampled for the FUSION layer\n",
    "                    if global_idx_item not in all_patient_zs_for_fusion:\n",
    "                        all_patient_zs_for_fusion[global_idx_item] = {}\n",
    "                    all_patient_zs_for_fusion[global_idx_item][view] = z_sampled[i]\n",
    "\n",
    "                    # Store mu_projected for the CONTRASTIVE loss\n",
    "                    if global_idx_item not in all_patient_mus_projected_for_cl:\n",
    "                        all_patient_mus_projected_for_cl[global_idx_item] = {}\n",
    "                    all_patient_mus_projected_for_cl[global_idx_item][view] = mu_projected[i]\n",
    "\n",
    "        # 1. FUSION: Use the 'z_sampled' embeddings\n",
    "        batch_fusion_input_list = []\n",
    "        for global_idx_tensor in batch_patient_global_indices:\n",
    "            global_idx_item = global_idx_tensor.item()\n",
    "            patient_embs = []\n",
    "            patient_z_data = all_patient_zs_for_fusion.get(global_idx_item, {})\n",
    "            for view in self.views:\n",
    "                emb = patient_z_data.get(view, torch.zeros(self.d_embed, device=device))\n",
    "                if view not in patient_z_data and self.missing_strategy == 'learnable':\n",
    "                    emb = self.missing_embeddings_params[view].squeeze(0)\n",
    "                patient_embs.append(emb)\n",
    "            batch_fusion_input_list.append(torch.stack(patient_embs))\n",
    "\n",
    "        batch_fusion_input_tensor = torch.stack(batch_fusion_input_list)\n",
    "        z_fused, fusion_attention = self.fusion_layer(batch_fusion_input_tensor)\n",
    "        logits = self.classifier(z_fused)\n",
    "\n",
    "        # 2. CONTRASTIVE LEARNING: Use the 'mu_projected' embeddings\n",
    "        mus_projected_for_cl_formatted = {}\n",
    "        for patient_idx, view_data in all_patient_mus_projected_for_cl.items():\n",
    "            for view, emb in view_data.items():\n",
    "                if view not in mus_projected_for_cl_formatted:\n",
    "                    mus_projected_for_cl_formatted[view] = {'embeddings': [], 'indices': []}\n",
    "                mus_projected_for_cl_formatted[view]['embeddings'].append(emb)\n",
    "                mus_projected_for_cl_formatted[view]['indices'].append(patient_idx)\n",
    "\n",
    "        final_mus_projected_for_cl = {\n",
    "            view: (torch.stack(data['embeddings']), torch.tensor(data['indices'], device=device))\n",
    "            for view, data in mus_projected_for_cl_formatted.items()\n",
    "        }\n",
    "\n",
    "        return logits, vae_outputs_for_loss, final_mus_projected_for_cl, fusion_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97670cb9-2e90-4905-93c6-afc0fc6d3476",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from scipy import interpolate\n",
    "\n",
    "def plot_kfold_roc_curves(roc_data_per_fold: List[Dict], title: str = \"ROC 10-CV\", save_path: Optional[str] = None):\n",
    "    \"\"\"\n",
    "    Generates a K-fold ROC plot with individual, mean, and merged curves.\n",
    "\n",
    "    Args:\n",
    "        roc_data_per_fold (List[Dict]): A list where each element is a dictionary\n",
    "            from a fold containing {'fpr', 'tpr', 'auc', 'y_true', 'y_pred'}.\n",
    "        title (str): The title of the plot.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(10, 9))\n",
    "    \n",
    "    tprs = []\n",
    "    aucs = []\n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "    \n",
    "    all_y_true = []\n",
    "    all_y_pred = []\n",
    "\n",
    "    # Plot individual fold ROC curves\n",
    "    for i, data in enumerate(roc_data_per_fold):\n",
    "        ax.plot(data['fpr'], data['tpr'], lw=1.5, alpha=0.6,\n",
    "                label=f\"Fold {i+1} (AUC = {data['auc']:.2f}), N={len(data['y_true'])} patients\")\n",
    "        \n",
    "        # For calculating the mean ROC\n",
    "        interp_tpr = interpolate.interp1d(data['fpr'], data['tpr'], kind='linear', bounds_error=False, fill_value=(0.0, 1.0))(mean_fpr)\n",
    "        tprs.append(interp_tpr)\n",
    "        aucs.append(data['auc'])\n",
    "        \n",
    "\n",
    "    # Plot chance line\n",
    "    ax.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r', label='Chance', alpha=0.8)\n",
    "\n",
    "    # Calculate and plot MEAN ROC\n",
    "    mean_tpr = np.mean(tprs, axis=0)\n",
    "    mean_tpr[-1] = 1.0\n",
    "    mean_auc = np.mean(aucs)\n",
    "    std_auc = np.std(aucs)\n",
    "    ax.plot(mean_fpr, mean_tpr, color='blue',\n",
    "            label=f'Mean ROC (AUC = {mean_auc:.2f} $\\\\pm$ {std_auc:.2f}), N={len(roc_data_per_fold)} folds',\n",
    "            lw=2.5, alpha=0.9)\n",
    "\n",
    "    # Plot standard deviation area\n",
    "    std_tpr = np.std(tprs, axis=0)\n",
    "    tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "    tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "    ax.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=0.3,\n",
    "                    label=r'$\\pm$ 1 std. dev.')\n",
    "\n",
    "    # Final plot settings\n",
    "    ax.set(xlim=[-0.05, 1.05], ylim=[-0.05, 1.05],\n",
    "           xlabel='False Positive Rate',\n",
    "           ylabel='True Positive Rate',\n",
    "           title=title)\n",
    "    ax.legend(loc=\"lower right\", fontsize=11)\n",
    "    ax.grid(alpha=0.5)\n",
    "\n",
    "    if save_path:\n",
    "        # Ensure the directory exists\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"   Plot saved to: {save_path}\")\n",
    "        \n",
    "    plt.show()\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b3134b-455e-409c-aa1f-fef6115aa03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports ---\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "from typing import Dict, Any, Tuple, Optional, List\n",
    "\n",
    "# --- Scikit-learn and PyTorch Geometric Imports ---\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score, roc_curve\n",
    "from torch_geometric.data import HeteroData\n",
    "\n",
    "# --- Assumed Helper Imports (ensure these are available) ---\n",
    "# from my_models import EndToEndMultiViewVAE_CL_AttentionRadiology, calculate_contrastive_loss\n",
    "\n",
    "# --- Helper function for linear annealing ---\n",
    "def linear_anneal(epoch: int, start_epoch: int, end_epoch: int, start_val: float, end_val: float) -> float:\n",
    "    \"\"\"Performs linear annealing.\"\"\"\n",
    "    if start_epoch >= end_epoch: return end_val\n",
    "    if epoch < start_epoch: return start_val\n",
    "    if epoch >= end_epoch: return end_val\n",
    "    return start_val + (end_val - start_val) * (epoch - start_epoch) / (end_epoch - start_epoch)\n",
    "\n",
    "\n",
    "# --- Main Training Function ---\n",
    "def kfold_train_attention_radiology_cl(\n",
    "    full_multi_view_data: HeteroData,\n",
    "    model_config: Dict,\n",
    "    train_config: Dict\n",
    ") -> Tuple[Dict[str, float], List[Dict]]:\n",
    "    \"\"\"\n",
    "    Performs K-fold cross-validation.\n",
    "    - Scheduler/Early Stopping monitor TOTAL VALIDATION LOSS.\n",
    "    - Best model state is saved based on highest VALIDATION AUC.\n",
    "    - Returns aggregated metrics and ROC data from the best AUC epoch.\n",
    "    \"\"\"\n",
    "    device = train_config['device']\n",
    "    full_multi_view_data = full_multi_view_data.to(device)\n",
    "\n",
    "    # --- Loss and Annealing Setup ---\n",
    "    criterion_bce_logits = nn.BCEWithLogitsLoss()\n",
    "    criterion_mse = nn.MSELoss()\n",
    "    loss_weights_config = train_config['loss_weights']\n",
    "    anneal_config = train_config.get('annealing', {})\n",
    "    \n",
    "    base_w_class = loss_weights_config['class']\n",
    "    base_w_cross_cl = loss_weights_config.get('cross_cl', 0.0)\n",
    "    base_w_kl = loss_weights_config['kl']\n",
    "    w_rec_attr_config = loss_weights_config['rec_attr']\n",
    "    w_rec_struct_config = loss_weights_config['rec_struct']\n",
    "    print('base_w_cross_cl: ', base_w_cross_cl)\n",
    "\n",
    "    kl_params = anneal_config.get('kl', {}); kl_start_w = kl_params.get('start_weight', base_w_kl); kl_end_w = kl_params.get('end_weight', base_w_kl); kl_start_e = kl_params.get('start_epoch', 0); kl_end_e = kl_params.get('end_epoch', 0)\n",
    "    cl_params = anneal_config.get('cross_cl', {}); cl_start_w = cl_params.get('start_weight', base_w_cross_cl); cl_end_w = cl_params.get('end_weight', base_w_cross_cl); cl_start_e = cl_params.get('start_epoch', 0); cl_end_e = cl_params.get('end_epoch', 0)\n",
    "\n",
    "    # --- Data Splitting ---\n",
    "    all_patient_indices_np = np.arange(full_multi_view_data['patient'].num_nodes)\n",
    "    y_for_stratification = full_multi_view_data['patient']['binary_label'].cpu().numpy()\n",
    "    kf = KFold(n_splits=train_config['n_splits'], shuffle=True, random_state=train_config.get('random_seed', 40))\n",
    "\n",
    "    fold_metrics_list = []\n",
    "    roc_data_per_fold = []\n",
    "    all_folds_detailed_logs = []\n",
    "\n",
    "    for fold, (train_global_idx_np, val_global_idx_np) in enumerate(kf.split(all_patient_indices_np)):\n",
    "        print(f\"\\n===== Fold {fold+1}/{train_config['n_splits']} =====\")\n",
    "        val_labels_for_check = y_for_stratification[val_global_idx_np]\n",
    "        unique_labels, counts = np.unique(val_labels_for_check, return_counts=True)\n",
    "        print(f\"  Fold {fold+1} Validation Label Distribution: {dict(zip(unique_labels, counts))}\")\n",
    "\n",
    "        if wandb_params:\n",
    "            full_wandb_config = {\n",
    "                **wandb_params['run_config'],\n",
    "                'fold': fold + 1,\n",
    "                'train_config': train_config,\n",
    "                'model_config': model_config \n",
    "            }\n",
    "            \n",
    "            wandb.init(\n",
    "                project=wandb_params['project_name'],\n",
    "                group=wandb_params['group_name'], \n",
    "                name=f\"fold-{fold+1}\",\n",
    "                config=full_wandb_config,\n",
    "                reinit=True \n",
    "            )\n",
    "\n",
    "        \n",
    "        train_fold_global_indices = torch.from_numpy(train_global_idx_np).to(device)\n",
    "        val_fold_global_indices = torch.from_numpy(val_global_idx_np).to(device)\n",
    "\n",
    "        model = EndToEndMultiViewVAE_CL_AttentionRadiology(\n",
    "            view_configs=model_config['view_configs'],\n",
    "            radiology_aggregator_config=model_config.get('radiology_aggregator_config'),\n",
    "            projection_head_config=model_config['projection_head_config'],\n",
    "            fusion_config=model_config['fusion_config'],\n",
    "            classifier_config=model_config['classifier_config'],\n",
    "            d_embed=model_config['d_embed'],\n",
    "            missing_strategy=model_config.get('missing_strategy', 'zero')\n",
    "        ).to(device)\n",
    "        \n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=train_config['lr'], weight_decay=train_config['wd'])\n",
    "        \n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, \n",
    "            mode='min', # We want to MINIMIZE loss\n",
    "            factor=0.5, \n",
    "            patience=train_config.get('patience', 10), \n",
    "            verbose=True\n",
    "        )\n",
    "\n",
    "        best_val_loss = float('inf')\n",
    "        best_val_auc_this_fold = -1.0\n",
    "        epochs_no_improve = 0 # Now based on loss improvement\n",
    "        best_model_state_fold = None\n",
    "        best_fold_roc_data = {}\n",
    "        fold_epoch_logs = []\n",
    "\n",
    "        for epoch in range(1, train_config['epochs'] + 1):\n",
    "            # --- Training Phase ---\n",
    "            model.train()\n",
    "            current_w_kl = linear_anneal(epoch, kl_start_e, kl_end_e, kl_start_w, kl_end_w)\n",
    "            current_w_cl = linear_anneal(epoch, cl_start_e, cl_end_e, cl_start_w, cl_end_w)\n",
    "            \n",
    "            # (Training loss calculation logic remains identical)\n",
    "            logits_train, vae_outputs_loss_train, mus_projected_for_cl_train, _ = model(full_multi_view_data, train_fold_global_indices)\n",
    "            true_labels_train = full_multi_view_data['patient']['binary_label'][train_fold_global_indices]\n",
    "            raw_loss_class_train = criterion_bce_logits(logits_train.squeeze(), true_labels_train.float())\n",
    "            raw_loss_cl_train = calculate_contrastive_loss(mus_projected_for_cl_train, train_config['cross_cl_temp'])\n",
    "            \n",
    "            total_loss_rec_attr_train, total_loss_rec_struct_train, total_loss_kl_train = 0.0, 0.0, 0.0\n",
    "            num_active_views_train = 0\n",
    "            for view_name in model.views:\n",
    "                vo = vae_outputs_loss_train.get(view_name, {})\n",
    "                if vo and vo.get('mu') is not None:\n",
    "                    num_active_views_train += 1\n",
    "                    w_attr = w_rec_attr_config if isinstance(w_rec_attr_config, float) else w_rec_attr_config.get(view_name, 1.0)\n",
    "                    total_loss_rec_attr_train += w_attr * criterion_mse(vo['rec_x'], vo['original_x_subset'])\n",
    "                    w_struct = w_rec_struct_config if isinstance(w_rec_struct_config, float) else w_rec_struct_config.get(view_name, 1.0)\n",
    "                    total_loss_rec_struct_train += w_struct * criterion_bce_logits(vo['rec_adj_logits'].reshape(-1), vo['original_adj_subset'].reshape(-1))\n",
    "                    kl_div = -0.5 * torch.sum(1 + vo['logvar'] - vo['mu'].pow(2) - vo['logvar'].exp(), dim=1).mean()\n",
    "                    total_loss_kl_train += kl_div\n",
    "            \n",
    "            avg_loss_rec_attr_train = total_loss_rec_attr_train / num_active_views_train if num_active_views_train > 0 else 0.0\n",
    "            avg_loss_rec_struct_train = total_loss_rec_struct_train / num_active_views_train if num_active_views_train > 0 else 0.0\n",
    "            avg_raw_loss_kl_train = total_loss_kl_train / num_active_views_train if num_active_views_train > 0 else 0.0\n",
    "            \n",
    "            total_train_loss = (base_w_class * raw_loss_class_train +\n",
    "                                current_w_cl * raw_loss_cl_train +\n",
    "                                avg_loss_rec_attr_train + \n",
    "                                avg_loss_rec_struct_train +\n",
    "                                current_w_kl * avg_raw_loss_kl_train)\n",
    "\n",
    "            epoch_log['train_total_loss'] = total_train_loss.item()\n",
    "            epoch_log['train_class_loss'] = raw_loss_class_train.item()\n",
    "            epoch_log['train_cl_loss'] = raw_loss_cl_train.item()\n",
    "            epoch_log['train_rec_attr_loss'] = avg_loss_rec_attr_train.item()\n",
    "            epoch_log['train_rec_struct_loss'] = avg_loss_rec_struct_train.item()\n",
    "            epoch_log['train_kl_loss'] = avg_raw_loss_kl_train.item()\n",
    "            epoch_log['w_kl'] = current_w_kl\n",
    "            epoch_log['w_cl'] = current_w_cl\n",
    "\n",
    "            if not torch.isnan(total_train_loss):\n",
    "                optimizer.zero_grad()\n",
    "                total_train_loss.backward()\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), max_norm=train_config.get('grad_clip_norm', 1.0))\n",
    "                optimizer.step()\n",
    "\n",
    "            # --- Validation Phase ---\n",
    "            model.eval()\n",
    "            total_validation_loss = torch.tensor(float('inf'), device=device)\n",
    "            current_val_auc = -1.0\n",
    "\n",
    "            if val_fold_global_indices.numel() > 0:\n",
    "                with torch.no_grad():\n",
    "                    val_logits_raw, vae_outputs_loss_val, mus_projected_for_cl_val, _ = model(full_multi_view_data, val_fold_global_indices)\n",
    "                    val_labels = full_multi_view_data['patient']['binary_label'][val_fold_global_indices]\n",
    "\n",
    "                    raw_loss_class_val = criterion_bce_logits(val_logits_raw.squeeze(), val_labels.float())\n",
    "                    raw_loss_cl_val = calculate_contrastive_loss(mus_projected_for_cl_val, train_config['cross_cl_temp'])\n",
    "                    total_loss_rec_attr_val, total_loss_rec_struct_val, total_loss_kl_val = 0.0, 0.0, 0.0\n",
    "                    num_active_views_val = 0\n",
    "                    for view_name in model.views:\n",
    "                        vo_val = vae_outputs_loss_val.get(view_name, {})\n",
    "                        if vo_val and vo_val.get('mu') is not None:\n",
    "                            num_active_views_val += 1\n",
    "                            w_attr = w_rec_attr_config if isinstance(w_rec_attr_config, float) else w_rec_attr_config.get(view_name, 1.0)\n",
    "                            total_loss_rec_attr_val += w_attr * criterion_mse(vo_val['rec_x'], vo_val['original_x_subset'])\n",
    "                            w_struct = w_rec_struct_config if isinstance(w_rec_struct_config, float) else w_rec_struct_config.get(view_name, 1.0)\n",
    "                            total_loss_rec_struct_val += w_struct * criterion_bce_logits(vo_val['rec_adj_logits'].reshape(-1), vo_val['original_adj_subset'].reshape(-1))\n",
    "                            kl_div_val = -0.5 * torch.sum(1 + vo_val['logvar'] - vo_val['mu'].pow(2) - vo_val['logvar'].exp(), dim=1).mean()\n",
    "                            total_loss_kl_val += kl_div_val\n",
    "                    \n",
    "                    avg_loss_rec_attr_val = total_loss_rec_attr_val / num_active_views_val if num_active_views_val > 0 else 0.0\n",
    "                    avg_loss_rec_struct_val = total_loss_rec_struct_val / num_active_views_val if num_active_views_val > 0 else 0.0\n",
    "                    avg_raw_loss_kl_val = total_loss_kl_val / num_active_views_val if num_active_views_val > 0 else 0.0\n",
    "                    \n",
    "                    # Use the final annealed weights for the validation loss to have a stable target\n",
    "                    total_validation_loss = (base_w_class * raw_loss_class_val +\n",
    "                                            cl_end_w * raw_loss_cl_val +\n",
    "                                            avg_loss_rec_attr_val + \n",
    "                                            avg_loss_rec_struct_val +\n",
    "                                            kl_end_w * avg_raw_loss_kl_val)\n",
    "\n",
    "                    if not torch.isnan(val_logits_raw).any() and len(np.unique(val_labels.cpu().numpy())) > 1:\n",
    "                        val_probs_np = torch.sigmoid(val_logits_raw.squeeze()).cpu().numpy()\n",
    "                        current_val_auc = roc_auc_score(val_labels.cpu().numpy(), val_probs_np)\n",
    "\n",
    "                    val_loss_metrics['val_total_loss'] = total_validation_loss.item()\n",
    "                    val_loss_metrics['val_class_loss'] = raw_loss_class_val.item()\n",
    "                    val_loss_metrics['val_cl_loss'] = raw_loss_cl_val.item()\n",
    "                    val_loss_metrics['val_rec_attr_loss'] = avg_loss_rec_attr_val.item()\n",
    "                    val_loss_metrics['val_rec_struct_loss'] = avg_loss_rec_struct_val.item()\n",
    "                    val_loss_metrics['val_kl_loss'] = avg_raw_loss_kl_val.item()\n",
    "                    val_loss_metrics['val_auc'] = current_val_auc\n",
    "\n",
    "            epoch_log.update(val_loss_metrics)\n",
    "            \n",
    "            if epoch % train_config.get('print_every_k_epochs', 10) == 0:\n",
    "                print(f\"  F{fold+1} Ep{epoch:03d} TLoss:{total_train_loss.item():.4f} | \"\n",
    "                      f\"VLoss:{total_validation_loss.item():.4f} (Best VLoss: {best_val_loss:.4f}) | \"\n",
    "                      f\"ValAUC:{current_val_auc:.4f} (Best ValAUC: {best_val_auc_this_fold:.4f})\")\n",
    "\n",
    "            # 1. Scheduler and Early Stopping are driven by Validation Loss\n",
    "            scheduler.step(total_validation_loss)\n",
    "            if total_validation_loss < best_val_loss:\n",
    "                best_val_loss = total_validation_loss\n",
    "                epochs_no_improve = 0\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "\n",
    "            # 2. Best Model State saving is driven by Validation AUC\n",
    "            if current_val_auc != -1.0 and current_val_auc > best_val_auc_this_fold:\n",
    "                best_val_auc_this_fold = current_val_auc\n",
    "                best_model_state_fold = model.state_dict().copy()\n",
    "                val_labels_np = val_labels.cpu().numpy()\n",
    "                fpr, tpr, _ = roc_curve(val_labels_np, val_probs_np)\n",
    "                best_fold_roc_data = {\n",
    "                    'fpr': fpr, 'tpr': tpr, 'auc': current_val_auc,\n",
    "                    'y_true': val_labels_np, 'y_pred': val_probs_np\n",
    "                }\n",
    "\n",
    "            fold_epoch_logs.append(epoch_log)\n",
    "            \n",
    "            if epochs_no_improve >= train_config.get('patience_early_stopping', 20):\n",
    "                print(f\"  Early stopping at epoch {epoch} for fold {fold+1} due to validation loss stagnation.\")\n",
    "                break\n",
    "        \n",
    "        # --- End of Fold ---\n",
    "        if best_fold_roc_data:\n",
    "            roc_data_per_fold.append(best_fold_roc_data)\n",
    "            y_true = best_fold_roc_data['y_true']\n",
    "            y_pred_probs = best_fold_roc_data['y_pred']\n",
    "            y_pred_binary = (y_pred_probs > 0.5).astype(int)\n",
    "\n",
    "            fold_results = {\n",
    "                'auc': best_fold_roc_data['auc'],\n",
    "                'f1': f1_score(y_true, y_pred_binary, zero_division=0),\n",
    "                'accuracy': accuracy_score(y_true, y_pred_binary),\n",
    "                'precision': precision_score(y_true, y_pred_binary, zero_division=0),\n",
    "                'recall': recall_score(y_true, y_pred_binary, zero_division=0)\n",
    "            }\n",
    "            fold_metrics_list.append(fold_results)\n",
    "            \n",
    "        else:\n",
    "           \n",
    "            fold_metrics_list.append({'auc': np.nan, 'f1': np.nan, 'accuracy': np.nan, 'precision': np.nan, 'recall': np.nan})\n",
    "        \n",
    "        del model, optimizer, scheduler, best_model_state_fold\n",
    "        if torch.cuda.is_available(): torch.cuda.empty_cache(); gc.collect()\n",
    "\n",
    "    if not fold_metrics_list:\n",
    "        print(\"Warning: No metrics were collected during cross-validation.\")\n",
    "        return {}, []\n",
    "\n",
    "    df_fold_metrics = pd.DataFrame(fold_metrics_list)\n",
    "    mean_metrics = df_fold_metrics.mean()\n",
    "    std_metrics = df_fold_metrics.std()\n",
    "    \n",
    "    results_summary = {}\n",
    "    for metric in ['auc', 'f1', 'accuracy', 'precision', 'recall']:\n",
    "        results_summary[f'mean_{metric}'] = mean_metrics.get(metric, np.nan)\n",
    "        results_summary[f'std_{metric}'] = std_metrics.get(metric, np.nan)\n",
    "\n",
    "    print(\"\\n--- Cross-Validation Summary (based on true best AUC epochs) ---\")\n",
    "    for key, value in results_summary.items():\n",
    "        print(f\"  {key}: {value:.4f}\")\n",
    "\n",
    "    return results_summary, df_fold_metrics, roc_data_per_fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9519bdc1-e488-462c-94ea-c317005b3ba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clinical Dim: 64, Path Dim: 137, Rad Dim: 1671\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "data = torch.load('multi_view_pdl1_data_lesions_247_thresh_07_robust.pt')\n",
    "DIM_CLINICAL = data['patient'].x_clinical.shape[1]\n",
    "DIM_PATHOLOGY = data['patient'].x_pathology.shape[1] # Dimension of GLCM features\n",
    "DIM_RADIOLOGY = data['lesion'].x.shape[1] \n",
    "print(f\"Clinical Dim: {DIM_CLINICAL}, Path Dim: {DIM_PATHOLOGY}, Rad Dim: {DIM_RADIOLOGY}\")\n",
    "# -------------------------------------------\n",
    "\n",
    "if any(d is None or d <= 0 for d in [DIM_CLINICAL, DIM_PATHOLOGY, DIM_RADIOLOGY]):\n",
    "     raise ValueError(\"Please determine and set the actual input dimensions (DIM_...)\")\n",
    "\n",
    "train_config_new = {\n",
    "    # --- Data and Device ---\n",
    "    'data_path': 'multi_view_pdl1_data_lesions_247_thresh_07_robust.pt', \n",
    "    'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "\n",
    "    # --- KFold and Epochs ---\n",
    "    'n_splits': 10,      \n",
    "    'epochs': 100,       \n",
    "    'patience': 10,\n",
    "    'patience_early_stopping': 35,\n",
    "    # --- Optimizer ---\n",
    "    'lr': 0.0001,        # Learning rate (might need to be smaller for complex models)\n",
    "    'wd': 1e-5,          # Weight decay for AdamW\n",
    "\n",
    "    # --- Loss Component Weights ---\n",
    "    # These are critical and require tuning!\n",
    "    'loss_weights': {\n",
    "        'class': 1.0,          # Weight for the main classification loss\n",
    "        'cross_cl': 0.5,    \n",
    "        'rec_struct': 0.1,    \n",
    "        'kl': 0.001,           \n",
    "    },\n",
    "    'annealing': {\n",
    "        'kl': {\n",
    "            'start_weight': 0.00001,\n",
    "            'end_weight': 0.001,     # Matches loss_weights['kl']\n",
    "            'start_epoch': 40,        # Start KL annealing from epoch 1\n",
    "            'end_epoch': 100          # Reach target KL weight by epoch 50\n",
    "        },\n",
    "        'cross_cl': {\n",
    "            'start_weight': 0.5,\n",
    "            'end_weight': 0.5,       # Matches loss_weights['cross_cl']\n",
    "            'start_epoch': 20,       \n",
    "            'end_epoch': 100\n",
    "        },\n",
    "    },\n",
    "    # --- Contrastive Learning Temperatures ---\n",
    "    'cross_cl_temp': 0.1,  # Temperature for cross-view contrastive loss\n",
    "    'intra_cl_temp': 0.1, \n",
    "    'grad_clip_norm': 1.0,\n",
    "    'print_epoch_freq': 1, \n",
    "}\n",
    "\n",
    "model_config_new = {\n",
    "    'view_configs': {\n",
    "        'clinical': {'in_channels': DIM_CLINICAL, 'hidden_channels_vae': 64, 'heads': 8, 'dropout': 0.5, 'num_gnn_layers': 2, 'edge_dim': 1},\n",
    "        'pathology': {'in_channels': DIM_PATHOLOGY, 'hidden_channels_vae': 64, 'heads': 8, 'dropout': 0.5, 'num_gnn_layers': 2, 'edge_dim': 1}, \n",
    "        'radiology': {\n",
    "            'in_channels': 128,  \n",
    "            'hidden_channels_vae': 64, \n",
    "            'heads': 8, 'dropout': 0.5, 'num_gnn_layers': 2, 'edge_dim': 1 # Patient-patient sim graph\n",
    "        },\n",
    "    },\n",
    "    'radiology_aggregator_config': {\n",
    "        'lesion_feature_dim': DIM_RADIOLOGY, # dim of one scaled lesion feature vector 1671\n",
    "        'aggregated_output_dim': 128, # Output dim of aggregator, input to VAE['radiology']\n",
    "        'attention_hidden_dim': 128,   # Hidden dim for attention MLP inside aggregator\n",
    "        'dropout': 0.5\n",
    "    },\n",
    "    'fusion_config': {\n",
    "        'hidden_dim_attention': 64,  # Hidden dimension for the Attention MLP within the fusion layer\n",
    "        'fused_dim': 64,             # Dimension AFTER fusion (should match d_embed if attention returns weighted sum)\n",
    "        'dropout_class': 0.5,          # Dropout for the final classifier MLP\n",
    "        'num_fusion_heads': 8,\n",
    "        'fusion_ffn_multiplier' : 2,\n",
    "        'num_fusion_transformer_layers': 2\n",
    "    },\n",
    "    'classifier_config': {            \n",
    "         'hidden_dim_classifier': 64\n",
    "    },\n",
    "    'projection_head_config': {\n",
    "        'hidden_dim': 64,    # Hidden dimension of the projection MLP\n",
    "        'output_dim': 64,    # Output dimension for contrastive learning (can be same as d_embed or different)\n",
    "        'dropout': 0.5\n",
    "    },\n",
    "    'd_embed': 64,                   \n",
    "    'missing_strategy': 'learnable'\n",
    "}\n",
    "\n",
    "full_multi_view_data = torch.load(train_config_new['data_path'])\n",
    "# results = kfold_train_attention_radiology_cl(full_multi_view_data, model_config_new, train_config_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24760be7-7844-482a-b319-fe4b4c2c93c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting K-Fold Cross-Validation...\n",
      "\n",
      "===== Fold 1/10 =====\n",
      "  Fold 1 Validation Label Distribution: {0: 8, 1: 17}\n",
      "  F1 Ep010 TLoss:6.0158 | VLoss:5.9845 (Best VLoss: 6.0601) | ValAUC:0.6912 (Best ValAUC: 0.6838)\n",
      "  F1 Ep020 TLoss:5.6368 | VLoss:5.1109 (Best VLoss: 5.1752) | ValAUC:0.6618 (Best ValAUC: 0.6985)\n",
      "  F1 Ep030 TLoss:5.3358 | VLoss:4.6901 (Best VLoss: 4.7251) | ValAUC:0.6471 (Best ValAUC: 0.6985)\n",
      "  F1 Ep040 TLoss:5.1301 | VLoss:4.5106 (Best VLoss: 4.5207) | ValAUC:0.6250 (Best ValAUC: 0.6985)\n",
      "  F1 Ep050 TLoss:4.9983 | VLoss:4.3873 (Best VLoss: 4.3902) | ValAUC:0.6029 (Best ValAUC: 0.6985)\n",
      "  F1 Ep060 TLoss:4.9496 | VLoss:4.3280 (Best VLoss: 4.3303) | ValAUC:0.5882 (Best ValAUC: 0.6985)\n",
      "  F1 Ep070 TLoss:4.9328 | VLoss:4.2138 (Best VLoss: 4.2242) | ValAUC:0.6029 (Best ValAUC: 0.6985)\n",
      "  F1 Ep080 TLoss:4.8585 | VLoss:4.1262 (Best VLoss: 4.1390) | ValAUC:0.6324 (Best ValAUC: 0.6985)\n",
      "  F1 Ep090 TLoss:4.8604 | VLoss:4.0900 (Best VLoss: 4.0859) | ValAUC:0.6324 (Best ValAUC: 0.6985)\n",
      "  F1 Ep100 TLoss:4.7622 | VLoss:4.0389 (Best VLoss: 4.0514) | ValAUC:0.6324 (Best ValAUC: 0.6985)\n",
      "  F1 Ep110 TLoss:4.6967 | VLoss:3.9655 (Best VLoss: 3.9639) | ValAUC:0.6176 (Best ValAUC: 0.6985)\n",
      "  F1 Ep120 TLoss:4.6545 | VLoss:3.9349 (Best VLoss: 3.9345) | ValAUC:0.6324 (Best ValAUC: 0.6985)\n",
      "  F1 Ep130 TLoss:4.6504 | VLoss:3.8865 (Best VLoss: 3.8912) | ValAUC:0.6324 (Best ValAUC: 0.6985)\n",
      "  F1 Ep140 TLoss:4.5732 | VLoss:3.9079 (Best VLoss: 3.8852) | ValAUC:0.6176 (Best ValAUC: 0.6985)\n",
      "  F1 Ep150 TLoss:4.4951 | VLoss:3.8808 (Best VLoss: 3.8764) | ValAUC:0.6176 (Best ValAUC: 0.6985)\n",
      "  F1 Ep160 TLoss:4.5071 | VLoss:3.8721 (Best VLoss: 3.8675) | ValAUC:0.6176 (Best ValAUC: 0.6985)\n",
      "  F1 Ep170 TLoss:4.4688 | VLoss:3.8311 (Best VLoss: 3.8419) | ValAUC:0.6103 (Best ValAUC: 0.6985)\n",
      "  F1 Ep180 TLoss:4.4267 | VLoss:3.7885 (Best VLoss: 3.7849) | ValAUC:0.6103 (Best ValAUC: 0.6985)\n",
      "  F1 Ep190 TLoss:4.4180 | VLoss:3.7642 (Best VLoss: 3.7736) | ValAUC:0.6176 (Best ValAUC: 0.6985)\n",
      "  F1 Ep200 TLoss:4.4007 | VLoss:3.7433 (Best VLoss: 3.7433) | ValAUC:0.6250 (Best ValAUC: 0.6985)\n",
      "\n",
      "===== Fold 2/10 =====\n",
      "  Fold 2 Validation Label Distribution: {0: 5, 1: 20}\n",
      "  F2 Ep010 TLoss:5.8880 | VLoss:5.6429 (Best VLoss: 5.7221) | ValAUC:0.8200 (Best ValAUC: 0.8300)\n",
      "  F2 Ep020 TLoss:5.5224 | VLoss:4.8229 (Best VLoss: 4.8702) | ValAUC:0.8000 (Best ValAUC: 0.8300)\n",
      "  F2 Ep030 TLoss:5.2537 | VLoss:4.3903 (Best VLoss: 4.4143) | ValAUC:0.7500 (Best ValAUC: 0.8300)\n",
      "  F2 Ep040 TLoss:5.1049 | VLoss:4.2166 (Best VLoss: 4.2293) | ValAUC:0.7800 (Best ValAUC: 0.8300)\n",
      "  F2 Ep050 TLoss:5.0158 | VLoss:4.0635 (Best VLoss: 4.0759) | ValAUC:0.7800 (Best ValAUC: 0.8300)\n",
      "  F2 Ep060 TLoss:4.9464 | VLoss:3.9681 (Best VLoss: 3.9780) | ValAUC:0.7900 (Best ValAUC: 0.8300)\n",
      "  F2 Ep070 TLoss:4.9115 | VLoss:3.8655 (Best VLoss: 3.8751) | ValAUC:0.7300 (Best ValAUC: 0.8300)\n",
      "  F2 Ep080 TLoss:4.8286 | VLoss:3.8255 (Best VLoss: 3.8361) | ValAUC:0.7400 (Best ValAUC: 0.8300)\n",
      "  F2 Ep090 TLoss:4.7841 | VLoss:3.7382 (Best VLoss: 3.7446) | ValAUC:0.7800 (Best ValAUC: 0.8300)\n",
      "  F2 Ep100 TLoss:4.7683 | VLoss:3.6419 (Best VLoss: 3.6441) | ValAUC:0.7900 (Best ValAUC: 0.8300)\n",
      "  F2 Ep110 TLoss:4.6843 | VLoss:3.6258 (Best VLoss: 3.6257) | ValAUC:0.7800 (Best ValAUC: 0.8300)\n",
      "  F2 Ep120 TLoss:4.6697 | VLoss:3.5671 (Best VLoss: 3.5773) | ValAUC:0.7900 (Best ValAUC: 0.8300)\n",
      "  F2 Ep130 TLoss:4.6349 | VLoss:3.5160 (Best VLoss: 3.5143) | ValAUC:0.7800 (Best ValAUC: 0.8300)\n",
      "  F2 Ep140 TLoss:4.5837 | VLoss:3.4649 (Best VLoss: 3.4728) | ValAUC:0.7900 (Best ValAUC: 0.8300)\n",
      "  F2 Ep150 TLoss:4.5300 | VLoss:3.4262 (Best VLoss: 3.4261) | ValAUC:0.7900 (Best ValAUC: 0.8300)\n",
      "  F2 Ep160 TLoss:4.5275 | VLoss:3.4026 (Best VLoss: 3.4055) | ValAUC:0.7900 (Best ValAUC: 0.8300)\n",
      "  F2 Ep170 TLoss:4.4327 | VLoss:3.3996 (Best VLoss: 3.3949) | ValAUC:0.7900 (Best ValAUC: 0.8300)\n",
      "  F2 Ep180 TLoss:4.4134 | VLoss:3.3798 (Best VLoss: 3.3813) | ValAUC:0.7800 (Best ValAUC: 0.8300)\n",
      "  F2 Ep190 TLoss:4.4492 | VLoss:3.3793 (Best VLoss: 3.3751) | ValAUC:0.7700 (Best ValAUC: 0.8300)\n",
      "Epoch 00195: reducing learning rate of group 0 to 5.0000e-05.\n",
      "  F2 Ep200 TLoss:4.3500 | VLoss:3.3811 (Best VLoss: 3.3728) | ValAUC:0.7700 (Best ValAUC: 0.8300)\n",
      "\n",
      "===== Fold 3/10 =====\n",
      "  Fold 3 Validation Label Distribution: {0: 8, 1: 17}\n",
      "  F3 Ep010 TLoss:6.0216 | VLoss:6.4820 (Best VLoss: 6.5624) | ValAUC:0.7500 (Best ValAUC: 0.7868)\n",
      "  F3 Ep020 TLoss:5.6358 | VLoss:5.7860 (Best VLoss: 5.8340) | ValAUC:0.6838 (Best ValAUC: 0.7868)\n",
      "  F3 Ep030 TLoss:5.3391 | VLoss:5.3297 (Best VLoss: 5.3713) | ValAUC:0.6691 (Best ValAUC: 0.7868)\n",
      "  F3 Ep040 TLoss:5.1602 | VLoss:4.9897 (Best VLoss: 5.0122) | ValAUC:0.6471 (Best ValAUC: 0.7868)\n",
      "  F3 Ep050 TLoss:5.1289 | VLoss:4.7424 (Best VLoss: 4.7676) | ValAUC:0.6544 (Best ValAUC: 0.7868)\n",
      "  F3 Ep060 TLoss:4.9771 | VLoss:4.5844 (Best VLoss: 4.5865) | ValAUC:0.6471 (Best ValAUC: 0.7868)\n",
      "  F3 Ep070 TLoss:4.8816 | VLoss:4.4200 (Best VLoss: 4.4385) | ValAUC:0.6324 (Best ValAUC: 0.7868)\n",
      "  F3 Ep080 TLoss:4.8480 | VLoss:4.2885 (Best VLoss: 4.2985) | ValAUC:0.6250 (Best ValAUC: 0.7868)\n",
      "  F3 Ep090 TLoss:4.8132 | VLoss:4.1580 (Best VLoss: 4.1756) | ValAUC:0.6471 (Best ValAUC: 0.7868)\n",
      "  F3 Ep100 TLoss:4.8476 | VLoss:4.0424 (Best VLoss: 4.0543) | ValAUC:0.6544 (Best ValAUC: 0.7868)\n",
      "  F3 Ep110 TLoss:4.7331 | VLoss:3.9750 (Best VLoss: 3.9807) | ValAUC:0.6544 (Best ValAUC: 0.7868)\n",
      "  F3 Ep120 TLoss:4.6621 | VLoss:3.9332 (Best VLoss: 3.9396) | ValAUC:0.6544 (Best ValAUC: 0.7868)\n",
      "  F3 Ep130 TLoss:4.6390 | VLoss:3.8920 (Best VLoss: 3.8930) | ValAUC:0.6691 (Best ValAUC: 0.7868)\n",
      "  F3 Ep140 TLoss:4.5666 | VLoss:3.8343 (Best VLoss: 3.8391) | ValAUC:0.6691 (Best ValAUC: 0.7868)\n",
      "  F3 Ep150 TLoss:4.5360 | VLoss:3.7895 (Best VLoss: 3.7908) | ValAUC:0.6691 (Best ValAUC: 0.7868)\n",
      "  F3 Ep160 TLoss:4.4922 | VLoss:3.7495 (Best VLoss: 3.7577) | ValAUC:0.6691 (Best ValAUC: 0.7868)\n",
      "  F3 Ep170 TLoss:4.4848 | VLoss:3.7492 (Best VLoss: 3.7483) | ValAUC:0.6691 (Best ValAUC: 0.7868)\n",
      "  F3 Ep180 TLoss:4.4619 | VLoss:3.7243 (Best VLoss: 3.7234) | ValAUC:0.6691 (Best ValAUC: 0.7868)\n",
      "  F3 Ep190 TLoss:4.4552 | VLoss:3.6886 (Best VLoss: 3.6943) | ValAUC:0.6618 (Best ValAUC: 0.7868)\n",
      "  F3 Ep200 TLoss:4.3984 | VLoss:3.6924 (Best VLoss: 3.6810) | ValAUC:0.6618 (Best ValAUC: 0.7868)\n",
      "\n",
      "===== Fold 4/10 =====\n",
      "  Fold 4 Validation Label Distribution: {0: 4, 1: 21}\n",
      "  F4 Ep010 TLoss:5.9089 | VLoss:6.0348 (Best VLoss: 6.1498) | ValAUC:0.7857 (Best ValAUC: 0.8333)\n",
      "  F4 Ep020 TLoss:5.5841 | VLoss:5.4103 (Best VLoss: 5.4405) | ValAUC:0.7857 (Best ValAUC: 0.8333)\n",
      "  F4 Ep030 TLoss:5.2401 | VLoss:5.3076 (Best VLoss: 5.3099) | ValAUC:0.8095 (Best ValAUC: 0.8333)\n",
      "  F4 Ep040 TLoss:5.1174 | VLoss:5.2920 (Best VLoss: 5.2974) | ValAUC:0.7857 (Best ValAUC: 0.8333)\n",
      "  F4 Ep050 TLoss:4.9994 | VLoss:5.2470 (Best VLoss: 5.2438) | ValAUC:0.7738 (Best ValAUC: 0.8333)\n",
      "  F4 Ep060 TLoss:4.9533 | VLoss:5.1872 (Best VLoss: 5.1934) | ValAUC:0.7857 (Best ValAUC: 0.8333)\n",
      "  F4 Ep070 TLoss:4.8856 | VLoss:5.0751 (Best VLoss: 5.0936) | ValAUC:0.7738 (Best ValAUC: 0.8333)\n",
      "  F4 Ep080 TLoss:4.8492 | VLoss:4.9866 (Best VLoss: 4.9899) | ValAUC:0.7976 (Best ValAUC: 0.8333)\n",
      "  F4 Ep090 TLoss:4.7858 | VLoss:4.8438 (Best VLoss: 4.8477) | ValAUC:0.8095 (Best ValAUC: 0.8333)\n",
      "  F4 Ep100 TLoss:4.8040 | VLoss:4.7667 (Best VLoss: 4.7646) | ValAUC:0.8095 (Best ValAUC: 0.8333)\n",
      "  F4 Ep110 TLoss:4.7289 | VLoss:4.7008 (Best VLoss: 4.7106) | ValAUC:0.7857 (Best ValAUC: 0.8333)\n",
      "  F4 Ep120 TLoss:4.6004 | VLoss:4.6210 (Best VLoss: 4.6223) | ValAUC:0.7857 (Best ValAUC: 0.8333)\n",
      "  F4 Ep130 TLoss:4.5707 | VLoss:4.5577 (Best VLoss: 4.5569) | ValAUC:0.7857 (Best ValAUC: 0.8333)\n",
      "  F4 Ep140 TLoss:4.5475 | VLoss:4.5197 (Best VLoss: 4.5162) | ValAUC:0.7976 (Best ValAUC: 0.8333)\n",
      "  F4 Ep150 TLoss:4.5382 | VLoss:4.4825 (Best VLoss: 4.4909) | ValAUC:0.7738 (Best ValAUC: 0.8333)\n",
      "  F4 Ep160 TLoss:4.4500 | VLoss:4.4602 (Best VLoss: 4.4613) | ValAUC:0.7500 (Best ValAUC: 0.8333)\n",
      "  F4 Ep170 TLoss:4.4829 | VLoss:4.3994 (Best VLoss: 4.4104) | ValAUC:0.7500 (Best ValAUC: 0.8333)\n",
      "  F4 Ep180 TLoss:4.4293 | VLoss:4.3366 (Best VLoss: 4.3357) | ValAUC:0.7262 (Best ValAUC: 0.8333)\n",
      "  F4 Ep190 TLoss:4.3883 | VLoss:4.3182 (Best VLoss: 4.3115) | ValAUC:0.7262 (Best ValAUC: 0.8333)\n",
      "Epoch 00199: reducing learning rate of group 0 to 5.0000e-05.\n",
      "  F4 Ep200 TLoss:4.4093 | VLoss:4.3377 (Best VLoss: 4.3115) | ValAUC:0.7143 (Best ValAUC: 0.8333)\n",
      "\n",
      "===== Fold 5/10 =====\n",
      "  Fold 5 Validation Label Distribution: {0: 7, 1: 18}\n",
      "  F5 Ep010 TLoss:6.0741 | VLoss:5.9939 (Best VLoss: 6.1227) | ValAUC:0.6111 (Best ValAUC: 0.6190)\n",
      "  F5 Ep020 TLoss:5.6905 | VLoss:5.1899 (Best VLoss: 5.2201) | ValAUC:0.5238 (Best ValAUC: 0.6190)\n",
      "  F5 Ep030 TLoss:5.4280 | VLoss:4.8667 (Best VLoss: 4.8902) | ValAUC:0.4286 (Best ValAUC: 0.6190)\n",
      "  F5 Ep040 TLoss:5.2370 | VLoss:4.7052 (Best VLoss: 4.7225) | ValAUC:0.3810 (Best ValAUC: 0.6190)\n",
      "  F5 Ep050 TLoss:5.1204 | VLoss:4.5251 (Best VLoss: 4.5356) | ValAUC:0.3175 (Best ValAUC: 0.6190)\n",
      "  F5 Ep060 TLoss:4.9773 | VLoss:4.4568 (Best VLoss: 4.4663) | ValAUC:0.3333 (Best ValAUC: 0.6190)\n",
      "  F5 Ep070 TLoss:4.9509 | VLoss:4.3810 (Best VLoss: 4.3959) | ValAUC:0.3254 (Best ValAUC: 0.6190)\n",
      "  F5 Ep080 TLoss:4.8678 | VLoss:4.2627 (Best VLoss: 4.2752) | ValAUC:0.3492 (Best ValAUC: 0.6190)\n",
      "  F5 Ep090 TLoss:4.8150 | VLoss:4.1717 (Best VLoss: 4.1780) | ValAUC:0.3968 (Best ValAUC: 0.6190)\n",
      "  F5 Ep100 TLoss:4.7997 | VLoss:4.1150 (Best VLoss: 4.1190) | ValAUC:0.4524 (Best ValAUC: 0.6190)\n",
      "  F5 Ep110 TLoss:4.7487 | VLoss:4.0315 (Best VLoss: 4.0505) | ValAUC:0.4603 (Best ValAUC: 0.6190)\n",
      "  F5 Ep120 TLoss:4.7355 | VLoss:3.9694 (Best VLoss: 3.9685) | ValAUC:0.4683 (Best ValAUC: 0.6190)\n",
      "  F5 Ep130 TLoss:4.6730 | VLoss:3.8958 (Best VLoss: 3.8983) | ValAUC:0.4365 (Best ValAUC: 0.6190)\n",
      "  F5 Ep140 TLoss:4.5837 | VLoss:3.8753 (Best VLoss: 3.8778) | ValAUC:0.4206 (Best ValAUC: 0.6190)\n",
      "  F5 Ep150 TLoss:4.5697 | VLoss:3.8256 (Best VLoss: 3.8279) | ValAUC:0.4286 (Best ValAUC: 0.6190)\n",
      "  F5 Ep160 TLoss:4.5395 | VLoss:3.8105 (Best VLoss: 3.8136) | ValAUC:0.4206 (Best ValAUC: 0.6190)\n",
      "  F5 Ep170 TLoss:4.4860 | VLoss:3.7919 (Best VLoss: 3.7864) | ValAUC:0.4127 (Best ValAUC: 0.6190)\n",
      "  F5 Ep180 TLoss:4.4256 | VLoss:3.7853 (Best VLoss: 3.7815) | ValAUC:0.4127 (Best ValAUC: 0.6190)\n",
      "  F5 Ep190 TLoss:4.4122 | VLoss:3.7707 (Best VLoss: 3.7728) | ValAUC:0.4048 (Best ValAUC: 0.6190)\n",
      "  F5 Ep200 TLoss:4.3709 | VLoss:3.7660 (Best VLoss: 3.7601) | ValAUC:0.4524 (Best ValAUC: 0.6190)\n",
      "\n",
      "===== Fold 6/10 =====\n",
      "  Fold 6 Validation Label Distribution: {0: 6, 1: 19}\n",
      "  F6 Ep010 TLoss:6.0293 | VLoss:6.4781 (Best VLoss: 6.5302) | ValAUC:0.5088 (Best ValAUC: 0.5175)\n",
      "  F6 Ep020 TLoss:5.6743 | VLoss:5.8252 (Best VLoss: 5.8882) | ValAUC:0.6491 (Best ValAUC: 0.6316)\n",
      "  F6 Ep030 TLoss:5.3846 | VLoss:5.4581 (Best VLoss: 5.4971) | ValAUC:0.6404 (Best ValAUC: 0.6579)\n",
      "  F6 Ep040 TLoss:5.1841 | VLoss:5.1147 (Best VLoss: 5.1471) | ValAUC:0.6140 (Best ValAUC: 0.6579)\n",
      "  F6 Ep050 TLoss:5.0268 | VLoss:4.8750 (Best VLoss: 4.9053) | ValAUC:0.5877 (Best ValAUC: 0.6579)\n",
      "  F6 Ep060 TLoss:4.9057 | VLoss:4.6823 (Best VLoss: 4.6942) | ValAUC:0.5614 (Best ValAUC: 0.6579)\n",
      "  F6 Ep070 TLoss:4.9038 | VLoss:4.5895 (Best VLoss: 4.6046) | ValAUC:0.5439 (Best ValAUC: 0.6579)\n",
      "  F6 Ep080 TLoss:4.8152 | VLoss:4.4639 (Best VLoss: 4.4661) | ValAUC:0.5614 (Best ValAUC: 0.6579)\n",
      "  F6 Ep090 TLoss:4.7891 | VLoss:4.3262 (Best VLoss: 4.3372) | ValAUC:0.5614 (Best ValAUC: 0.6579)\n",
      "  F6 Ep100 TLoss:4.7651 | VLoss:4.2241 (Best VLoss: 4.2313) | ValAUC:0.5614 (Best ValAUC: 0.6579)\n",
      "  F6 Ep110 TLoss:4.7206 | VLoss:4.1381 (Best VLoss: 4.1442) | ValAUC:0.5614 (Best ValAUC: 0.6579)\n",
      "  F6 Ep120 TLoss:4.6780 | VLoss:4.0593 (Best VLoss: 4.0650) | ValAUC:0.5526 (Best ValAUC: 0.6579)\n",
      "  F6 Ep130 TLoss:4.6423 | VLoss:3.9928 (Best VLoss: 3.9934) | ValAUC:0.5439 (Best ValAUC: 0.6579)\n",
      "  F6 Ep140 TLoss:4.5464 | VLoss:3.9148 (Best VLoss: 3.9180) | ValAUC:0.5351 (Best ValAUC: 0.6579)\n",
      "  F6 Ep150 TLoss:4.5212 | VLoss:3.8983 (Best VLoss: 3.8977) | ValAUC:0.5263 (Best ValAUC: 0.6579)\n",
      "  F6 Ep160 TLoss:4.4746 | VLoss:3.8689 (Best VLoss: 3.8590) | ValAUC:0.5263 (Best ValAUC: 0.6579)\n",
      "  F6 Ep170 TLoss:4.4500 | VLoss:3.8589 (Best VLoss: 3.8519) | ValAUC:0.5175 (Best ValAUC: 0.6579)\n",
      "  F6 Ep180 TLoss:4.3954 | VLoss:3.8368 (Best VLoss: 3.8415) | ValAUC:0.5088 (Best ValAUC: 0.6579)\n",
      "  F6 Ep190 TLoss:4.3851 | VLoss:3.7800 (Best VLoss: 3.7898) | ValAUC:0.5088 (Best ValAUC: 0.6579)\n",
      "  F6 Ep200 TLoss:4.3684 | VLoss:3.7416 (Best VLoss: 3.7425) | ValAUC:0.5175 (Best ValAUC: 0.6579)\n",
      "\n",
      "===== Fold 7/10 =====\n",
      "  Fold 7 Validation Label Distribution: {0: 9, 1: 16}\n",
      "  F7 Ep010 TLoss:5.8156 | VLoss:5.4922 (Best VLoss: 5.5762) | ValAUC:0.7500 (Best ValAUC: 0.7431)\n",
      "  F7 Ep020 TLoss:5.5276 | VLoss:4.8218 (Best VLoss: 4.8612) | ValAUC:0.7708 (Best ValAUC: 0.7639)\n",
      "  F7 Ep030 TLoss:5.2330 | VLoss:4.5325 (Best VLoss: 4.5593) | ValAUC:0.8125 (Best ValAUC: 0.8125)\n",
      "  F7 Ep040 TLoss:5.0779 | VLoss:4.3337 (Best VLoss: 4.3469) | ValAUC:0.7986 (Best ValAUC: 0.8125)\n",
      "  F7 Ep050 TLoss:4.9511 | VLoss:4.1742 (Best VLoss: 4.1832) | ValAUC:0.8056 (Best ValAUC: 0.8125)\n",
      "  F7 Ep060 TLoss:4.8797 | VLoss:4.0651 (Best VLoss: 4.0736) | ValAUC:0.7986 (Best ValAUC: 0.8125)\n",
      "  F7 Ep070 TLoss:4.8319 | VLoss:3.9741 (Best VLoss: 3.9855) | ValAUC:0.7986 (Best ValAUC: 0.8125)\n",
      "  F7 Ep080 TLoss:4.8776 | VLoss:3.8863 (Best VLoss: 3.8954) | ValAUC:0.8125 (Best ValAUC: 0.8125)\n",
      "  F7 Ep090 TLoss:4.8363 | VLoss:3.7938 (Best VLoss: 3.7984) | ValAUC:0.8125 (Best ValAUC: 0.8125)\n",
      "  F7 Ep100 TLoss:4.7971 | VLoss:3.7323 (Best VLoss: 3.7385) | ValAUC:0.8125 (Best ValAUC: 0.8125)\n",
      "  F7 Ep110 TLoss:4.7313 | VLoss:3.6571 (Best VLoss: 3.6619) | ValAUC:0.8125 (Best ValAUC: 0.8125)\n",
      "  F7 Ep120 TLoss:4.6767 | VLoss:3.6066 (Best VLoss: 3.6089) | ValAUC:0.8056 (Best ValAUC: 0.8125)\n",
      "  F7 Ep130 TLoss:4.6193 | VLoss:3.5515 (Best VLoss: 3.5583) | ValAUC:0.8056 (Best ValAUC: 0.8125)\n",
      "  F7 Ep140 TLoss:4.5572 | VLoss:3.5190 (Best VLoss: 3.5200) | ValAUC:0.7986 (Best ValAUC: 0.8125)\n",
      "  F7 Ep150 TLoss:4.5700 | VLoss:3.4852 (Best VLoss: 3.4859) | ValAUC:0.7986 (Best ValAUC: 0.8125)\n",
      "  F7 Ep160 TLoss:4.4945 | VLoss:3.4553 (Best VLoss: 3.4596) | ValAUC:0.7986 (Best ValAUC: 0.8125)\n",
      "  F7 Ep170 TLoss:4.5066 | VLoss:3.3917 (Best VLoss: 3.3973) | ValAUC:0.7986 (Best ValAUC: 0.8125)\n",
      "  F7 Ep180 TLoss:4.4583 | VLoss:3.3674 (Best VLoss: 3.3679) | ValAUC:0.7986 (Best ValAUC: 0.8125)\n",
      "  F7 Ep190 TLoss:4.4098 | VLoss:3.3372 (Best VLoss: 3.3368) | ValAUC:0.7986 (Best ValAUC: 0.8125)\n",
      "  F7 Ep200 TLoss:4.3535 | VLoss:3.3255 (Best VLoss: 3.3265) | ValAUC:0.7986 (Best ValAUC: 0.8125)\n",
      "\n",
      "===== Fold 8/10 =====\n",
      "  Fold 8 Validation Label Distribution: {0: 7, 1: 17}\n",
      "  F8 Ep010 TLoss:6.1395 | VLoss:6.0839 (Best VLoss: 6.1883) | ValAUC:0.6555 (Best ValAUC: 0.6723)\n",
      "  F8 Ep020 TLoss:5.7143 | VLoss:5.3638 (Best VLoss: 5.4241) | ValAUC:0.6639 (Best ValAUC: 0.6723)\n",
      "  F8 Ep030 TLoss:5.4297 | VLoss:4.8873 (Best VLoss: 4.9322) | ValAUC:0.6891 (Best ValAUC: 0.6807)\n",
      "  F8 Ep040 TLoss:5.1318 | VLoss:4.6432 (Best VLoss: 4.6586) | ValAUC:0.6639 (Best ValAUC: 0.6975)\n",
      "  F8 Ep050 TLoss:5.1243 | VLoss:4.4881 (Best VLoss: 4.4929) | ValAUC:0.7479 (Best ValAUC: 0.7479)\n",
      "  F8 Ep060 TLoss:4.9639 | VLoss:4.3836 (Best VLoss: 4.3874) | ValAUC:0.7899 (Best ValAUC: 0.7815)\n",
      "  F8 Ep070 TLoss:4.9740 | VLoss:4.3043 (Best VLoss: 4.3109) | ValAUC:0.8235 (Best ValAUC: 0.8151)\n",
      "  F8 Ep080 TLoss:4.9396 | VLoss:4.2579 (Best VLoss: 4.2672) | ValAUC:0.8319 (Best ValAUC: 0.8235)\n",
      "  F8 Ep090 TLoss:4.8414 | VLoss:4.1959 (Best VLoss: 4.2000) | ValAUC:0.8319 (Best ValAUC: 0.8319)\n",
      "  F8 Ep100 TLoss:4.8210 | VLoss:4.0553 (Best VLoss: 4.0832) | ValAUC:0.8487 (Best ValAUC: 0.8487)\n",
      "  F8 Ep110 TLoss:4.7490 | VLoss:3.9478 (Best VLoss: 3.9573) | ValAUC:0.8403 (Best ValAUC: 0.8487)\n",
      "  F8 Ep120 TLoss:4.7066 | VLoss:3.8649 (Best VLoss: 3.8679) | ValAUC:0.8487 (Best ValAUC: 0.8487)\n",
      "  F8 Ep130 TLoss:4.6551 | VLoss:3.7833 (Best VLoss: 3.7903) | ValAUC:0.8487 (Best ValAUC: 0.8571)\n",
      "  F8 Ep140 TLoss:4.5870 | VLoss:3.7011 (Best VLoss: 3.7094) | ValAUC:0.8571 (Best ValAUC: 0.8571)\n",
      "  F8 Ep150 TLoss:4.6031 | VLoss:3.6827 (Best VLoss: 3.6850) | ValAUC:0.8655 (Best ValAUC: 0.8655)\n",
      "  F8 Ep160 TLoss:4.5422 | VLoss:3.6565 (Best VLoss: 3.6605) | ValAUC:0.8571 (Best ValAUC: 0.8655)\n",
      "  F8 Ep170 TLoss:4.5308 | VLoss:3.5905 (Best VLoss: 3.5987) | ValAUC:0.8487 (Best ValAUC: 0.8655)\n",
      "  F8 Ep180 TLoss:4.4885 | VLoss:3.5068 (Best VLoss: 3.5213) | ValAUC:0.8655 (Best ValAUC: 0.8655)\n",
      "  F8 Ep190 TLoss:4.4140 | VLoss:3.4386 (Best VLoss: 3.4464) | ValAUC:0.8655 (Best ValAUC: 0.8655)\n",
      "  F8 Ep200 TLoss:4.3772 | VLoss:3.4452 (Best VLoss: 3.4386) | ValAUC:0.8571 (Best ValAUC: 0.8655)\n",
      "\n",
      "===== Fold 9/10 =====\n",
      "  Fold 9 Validation Label Distribution: {0: 5, 1: 19}\n",
      "  F9 Ep010 TLoss:6.3541 | VLoss:6.3538 (Best VLoss: 6.4895) | ValAUC:0.5789 (Best ValAUC: 0.6105)\n",
      "  F9 Ep020 TLoss:5.8602 | VLoss:5.5114 (Best VLoss: 5.5756) | ValAUC:0.6211 (Best ValAUC: 0.6211)\n",
      "  F9 Ep030 TLoss:5.4484 | VLoss:5.0347 (Best VLoss: 5.0676) | ValAUC:0.6737 (Best ValAUC: 0.6737)\n",
      "  F9 Ep040 TLoss:5.2532 | VLoss:4.7964 (Best VLoss: 4.8127) | ValAUC:0.6737 (Best ValAUC: 0.6737)\n",
      "  F9 Ep050 TLoss:5.1622 | VLoss:4.5648 (Best VLoss: 4.5977) | ValAUC:0.7053 (Best ValAUC: 0.7053)\n",
      "  F9 Ep060 TLoss:5.0307 | VLoss:4.3277 (Best VLoss: 4.3518) | ValAUC:0.6737 (Best ValAUC: 0.7053)\n",
      "  F9 Ep070 TLoss:4.9342 | VLoss:4.1887 (Best VLoss: 4.1991) | ValAUC:0.6842 (Best ValAUC: 0.7053)\n",
      "  F9 Ep080 TLoss:4.8854 | VLoss:4.0533 (Best VLoss: 4.0697) | ValAUC:0.6632 (Best ValAUC: 0.7053)\n",
      "  F9 Ep090 TLoss:4.8986 | VLoss:3.9401 (Best VLoss: 3.9378) | ValAUC:0.6526 (Best ValAUC: 0.7053)\n",
      "  F9 Ep100 TLoss:4.7961 | VLoss:3.8784 (Best VLoss: 3.8899) | ValAUC:0.6632 (Best ValAUC: 0.7053)\n",
      "  F9 Ep110 TLoss:4.7349 | VLoss:3.8209 (Best VLoss: 3.8309) | ValAUC:0.6632 (Best ValAUC: 0.7053)\n",
      "  F9 Ep120 TLoss:4.7387 | VLoss:3.7658 (Best VLoss: 3.7702) | ValAUC:0.6526 (Best ValAUC: 0.7053)\n",
      "  F9 Ep130 TLoss:4.6558 | VLoss:3.6890 (Best VLoss: 3.6965) | ValAUC:0.6526 (Best ValAUC: 0.7053)\n",
      "  F9 Ep140 TLoss:4.6407 | VLoss:3.6458 (Best VLoss: 3.6501) | ValAUC:0.6421 (Best ValAUC: 0.7053)\n",
      "  F9 Ep150 TLoss:4.5668 | VLoss:3.6096 (Best VLoss: 3.6106) | ValAUC:0.6526 (Best ValAUC: 0.7053)\n",
      "  F9 Ep160 TLoss:4.5238 | VLoss:3.5731 (Best VLoss: 3.5766) | ValAUC:0.6526 (Best ValAUC: 0.7053)\n",
      "  F9 Ep170 TLoss:4.5156 | VLoss:3.5587 (Best VLoss: 3.5524) | ValAUC:0.6526 (Best ValAUC: 0.7053)\n",
      "  F9 Ep180 TLoss:4.4608 | VLoss:3.5709 (Best VLoss: 3.5524) | ValAUC:0.6632 (Best ValAUC: 0.7053)\n",
      "Epoch 00180: reducing learning rate of group 0 to 5.0000e-05.\n",
      "  F9 Ep190 TLoss:4.4477 | VLoss:3.5663 (Best VLoss: 3.5524) | ValAUC:0.6526 (Best ValAUC: 0.7053)\n",
      "Epoch 00191: reducing learning rate of group 0 to 2.5000e-05.\n",
      "  F9 Ep200 TLoss:4.4288 | VLoss:3.5658 (Best VLoss: 3.5524) | ValAUC:0.6526 (Best ValAUC: 0.7053)\n",
      "\n",
      "===== Fold 10/10 =====\n",
      "  Fold 10 Validation Label Distribution: {0: 3, 1: 21}\n",
      "  F10 Ep010 TLoss:6.0188 | VLoss:6.2301 (Best VLoss: 6.3110) | ValAUC:0.3968 (Best ValAUC: 0.3492)\n",
      "  F10 Ep020 TLoss:5.6367 | VLoss:5.4867 (Best VLoss: 5.5392) | ValAUC:0.6508 (Best ValAUC: 0.6508)\n",
      "  F10 Ep030 TLoss:5.3802 | VLoss:4.8977 (Best VLoss: 4.9425) | ValAUC:0.6508 (Best ValAUC: 0.6667)\n",
      "  F10 Ep040 TLoss:5.1623 | VLoss:4.6190 (Best VLoss: 4.6473) | ValAUC:0.6825 (Best ValAUC: 0.6825)\n",
      "  F10 Ep050 TLoss:5.0553 | VLoss:4.4039 (Best VLoss: 4.4165) | ValAUC:0.7778 (Best ValAUC: 0.7778)\n",
      "  F10 Ep060 TLoss:4.9438 | VLoss:4.2988 (Best VLoss: 4.3036) | ValAUC:0.8095 (Best ValAUC: 0.8095)\n",
      "  F10 Ep070 TLoss:4.9029 | VLoss:4.1956 (Best VLoss: 4.1958) | ValAUC:0.8413 (Best ValAUC: 0.8254)\n",
      "  F10 Ep080 TLoss:4.8414 | VLoss:4.1082 (Best VLoss: 4.1142) | ValAUC:0.8413 (Best ValAUC: 0.8571)\n",
      "  F10 Ep090 TLoss:4.7922 | VLoss:4.0691 (Best VLoss: 4.0763) | ValAUC:0.8730 (Best ValAUC: 0.8730)\n",
      "  F10 Ep100 TLoss:4.7628 | VLoss:4.0079 (Best VLoss: 4.0152) | ValAUC:0.9048 (Best ValAUC: 0.9048)\n",
      "  F10 Ep110 TLoss:4.7306 | VLoss:3.9304 (Best VLoss: 3.9285) | ValAUC:0.9048 (Best ValAUC: 0.9048)\n",
      "  F10 Ep120 TLoss:4.7477 | VLoss:3.8500 (Best VLoss: 3.8657) | ValAUC:0.9048 (Best ValAUC: 0.9048)\n",
      "  F10 Ep130 TLoss:4.6346 | VLoss:3.8238 (Best VLoss: 3.8237) | ValAUC:0.9048 (Best ValAUC: 0.9048)\n",
      "  F10 Ep140 TLoss:4.5947 | VLoss:3.7619 (Best VLoss: 3.7631) | ValAUC:0.9048 (Best ValAUC: 0.9048)\n",
      "  F10 Ep150 TLoss:4.5366 | VLoss:3.7149 (Best VLoss: 3.7337) | ValAUC:0.9048 (Best ValAUC: 0.9048)\n",
      "  F10 Ep160 TLoss:4.5153 | VLoss:3.6353 (Best VLoss: 3.6416) | ValAUC:0.9048 (Best ValAUC: 0.9048)\n",
      "  F10 Ep170 TLoss:4.4690 | VLoss:3.6314 (Best VLoss: 3.6298) | ValAUC:0.9048 (Best ValAUC: 0.9048)\n",
      "  F10 Ep180 TLoss:4.4310 | VLoss:3.6174 (Best VLoss: 3.6181) | ValAUC:0.8889 (Best ValAUC: 0.9048)\n",
      "  F10 Ep190 TLoss:4.4470 | VLoss:3.5873 (Best VLoss: 3.5781) | ValAUC:0.8889 (Best ValAUC: 0.9048)\n",
      "  F10 Ep200 TLoss:4.4308 | VLoss:3.5536 (Best VLoss: 3.5507) | ValAUC:0.8889 (Best ValAUC: 0.9048)\n",
      "\n",
      "--- Cross-Validation Summary (based on true best AUCs) ---\n",
      "  mean_auc: 0.7714\n",
      "  std_auc: 0.0952\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting K-Fold Cross-Validation...\")\n",
    "results_summary, df_fold_metrics, roc_data_per_fold = kfold_train_attention_radiology_cl(\n",
    "    full_multi_view_data, \n",
    "    model_config_new, \n",
    "    train_config_new\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
